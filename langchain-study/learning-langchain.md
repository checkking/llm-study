# LangChain å­¦ä¹ 
## LangChain æ¦‚è¿°
LangChain æä¾›äº†é¢„æ„å»ºçš„æ™ºèƒ½ä½“æ¶æ„å’Œæ¨¡å‹é›†æˆï¼Œå¸®åŠ©æˆ‘ä»¬å¿«é€Ÿä¸Šæ‰‹ï¼Œæ— ç¼åœ°å°†å¤§è¯­è¨€æ¨¡å‹èå…¥æ™ºèƒ½ä½“å’Œåº”ç”¨ä¸­ã€‚
å¦‚æœæ‚¨å¸Œæœ›å¿«é€Ÿæ„å»ºæ™ºèƒ½ä½“å’Œè‡ªä¸»åº”ç”¨ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨ä½¿ç”¨LangChainã€‚å½“æ‚¨æœ‰æ›´é«˜çº§çš„éœ€æ±‚ï¼Œéœ€è¦ç»“åˆç¡®å®šæ€§å·¥ä½œæµä¸æ™ºèƒ½ä½“å·¥ä½œæµã€è¿›è¡Œæ·±åº¦å®šåˆ¶å¹¶ä¸¥æ ¼æ§åˆ¶å»¶è¿Ÿæ—¶ï¼Œè¯·ä½¿ç”¨LangGraphâ€”â€”æˆ‘ä»¬çš„ä½çº§æ™ºèƒ½ä½“ç¼–æ’æ¡†æ¶å’Œè¿è¡Œæ—¶ã€‚

LangChainä»£ç†æ„å»ºäºLangGraphä¹‹ä¸Šï¼Œä»¥æä¾›æŒä¹…æ‰§è¡Œã€æµå¼å¤„ç†ã€äººæœºååŒã€æŒä¹…åŒ–ç­‰åŠŸèƒ½ã€‚æ‚¨æ— éœ€äº†è§£LangGraphï¼Œå³å¯è½»æ¾ä½¿ç”¨åŸºæœ¬çš„LangChainä»£ç†ã€‚

## Langchain å®‰è£…
```bash
pip install -q langchain
```

## LangChain åŸºç¡€æ™ºèƒ½ä½“å®ç°

```
from langchain.agents import create_agent

def get_weather(city: str) -> str:
    """Get weather for a given city."""
    return f"It's always sunny in {city}!"

agent = create_agent(
    model="claude-sonnet-4-5-20250929",
    tools=[get_weather],
    system_prompt="You are a helpful assistant",
)

# Run the agent
agent.invoke(
    {"messages": [{"role": "user", "content": "what is the weather in sf"}]}
)
```

ä¸Šé¢çš„æ–¹å¼éœ€è¦æœ¬åœ°éƒ¨ç½²å¥½å¤§æ¨¡å‹ï¼Œæˆ–è€…ç”¨ Claude (Anthropic)çš„API KEYè´¦å·ï¼Œ å¹¶è®¾ç½®ANTHROPIC_API_KEYç¯å¢ƒå˜é‡ã€‚å› æ¡ä»¶åŸå› ï¼Œä¸Šé¢ä»£ç ä¸èƒ½è¿è¡Œã€‚

## LangChain æ™ºèƒ½ä½“å®ç°ï¼ˆOpenAIï¼‰

```
# 1. å¯¼å…¥å¿…è¦çš„æ¨¡å—
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent


# 2. å°†å‡½æ•°å®šä¹‰ä¸º LangChain çš„ Tool
def get_weather(city: str) -> str:
    """Get the current weather in a given city."""
    # è¿™é‡Œæ˜¯æ¨¡æ‹Ÿï¼ŒçœŸå®åº”ç”¨ä¸­åº”æ¥å…¥å¤©æ°”API
    return f"The weather in {city} is sunny and 72Â°F."

# 3. ä»¥ OpenAI å…¼å®¹æ¨¡å¼åˆå§‹åŒ–èŠå¤©æ¨¡å‹
# å…³é”®ï¼šé€šè¿‡æŒ‡å®š base_url æ¥å…¼å®¹ä¸åŒæœåŠ¡å•†
llm = ChatOpenAI(
    model="deepseek-chat",  # æ›¿æ¢ä¸ºè±†åŒ…çš„æ¨¡å‹åï¼Œå¦‚ "Doubao-lite-32k"
    openai_api_key="{your key}",  # æ›¿æ¢ä¸ºä½ çš„APIå¯†é’¥
    base_url="https://api.deepseek.com/v1",  # æ›¿æ¢ä¸ºè±†åŒ…çš„APIç«¯ç‚¹
    temperature=0
)

# 4. å®šä¹‰æ™ºèƒ½ä½“çš„ç³»ç»Ÿæç¤ºè¯
system_prompt = "You are a helpful assistant that can provide weather information."

# 5. åˆ›å»ºæ™ºèƒ½ä½“
agent = create_agent(
    model=llm,
    tools=[get_weather],
    system_prompt=system_prompt
)

# 6. æ‰§è¡ŒæŸ¥è¯¢
result = agent.invoke(
    {"messages": [{"role": "user", "content": "What is the weather in San Francisco?"}]}
)

print(result)
```

æ³¨æ„ï¼š å¯†é’¥ä¸è¦æ”¾åˆ°ä»£ç ä¸­ï¼Œåº”è¯¥ä»ç¯å¢ƒå˜é‡ä¸­è·å–ã€‚ å¦‚æœç”¨condaç®¡ç†ç¯å¢ƒï¼Œå¯ä»¥åœ¨å½“å‰ç¯å¢ƒä¸‹è®¾ç½®ç¯å¢ƒå˜é‡.

```
conda env config vars set OPENAI_API_KEY={your key}
```

è®¾ç½®å®Œæˆåï¼Œå¿…é¡»å…ˆåœç”¨å†é‡æ–°æ¿€æ´»ç¯å¢ƒï¼Œå˜é‡æ‰ä¼šåŠ è½½ï¼š
```
conda deactivate
conda activate {your env}
```

### ç«å±±å¼•æ“Doubaoæ¨¡å‹æ¥å…¥å…³é”®ä»£ç 

```
llm_doubao = ChatOpenAI(
    model="doubao-1-5-pro-32k-250115",  # æ›¿æ¢ä¸ºè±†åŒ…çš„æ¨¡å‹åï¼Œå¦‚ "Doubao-lite-32k"
    openai_api_key=os.environ["ARK_OPENAI_API_KEY"],  # æ›¿æ¢ä¸ºä½ çš„APIå¯†é’¥
    base_url="https://ark.cn-beijing.volces.com/api/v3",  # æ›¿æ¢ä¸ºç«å±±å¼•æ“çš„APIç«¯ç‚¹
    temperature=0
)

system_prompt = "You are a helpful assistant that can provide weather information."

agent_doubao = create_agent(
    model=llm_doubao,
    tools=[get_weather],
    system_prompt=system_prompt
)

result_doubao = agent_doubao.invoke(
    {"messages": [{"role": "user", "content": "What is the weather in San Francisco?"}]}
)
print(result_doubao)
```

## åˆ›å»ºä¸€ä¸ªçœŸå®ä¸–ç•Œçš„æ™ºèƒ½ä½“
æ„å»ºä¸€ä¸ªå®ç”¨çš„å¤©æ°”é¢„æŠ¥ä»£ç†ï¼Œä»¥å±•ç¤ºå…³é”®çš„ç”Ÿäº§æ¦‚å¿µï¼š

1. ç»†åŒ–ç³»ç»Ÿæç¤ºä»¥æ”¹å–„æ™ºèƒ½ä½“è¡Œä¸º
2. åˆ›å»ºå¯ä¸å¤–éƒ¨æ•°æ®é›†æˆçš„å·¥å…·
3. æ¨¡å‹é…ç½®ä»¥å®ç°ä¸€è‡´çš„å›å¤
4. ç»“æ„åŒ–è¾“å‡ºä»¥è·å¾—å¯é¢„æµ‹çš„ç»“æœ
5. å¯¹è¯è®°å¿†ç”¨äºèŠå¤©å¼äº¤äº’
6. åˆ›å»ºå¹¶è¿è¡Œæ™ºèƒ½ä½“åˆ›å»ºä¸€ä¸ªåŠŸèƒ½å®Œå¤‡çš„æ™ºèƒ½ä½“

### å®šä¹‰ç³»ç»Ÿæç¤ºè¯
ç³»ç»Ÿæç¤ºè¯å®šä¹‰æ™ºèƒ½ä½“çš„è§’è‰²å’Œè¡Œä¸ºï¼Œä¿æŒå…·ä½“ä¸”å¯æ“ä½œï¼š
```
SYSTEM_PROMPT = """You are an expert weather forecaster, who speaks in puns.

You have access to two tools:

- get_weather_for_location: use this to get the weather for a specific location
- get_user_location: use this to get the user's location

If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location."""
```

### åˆ›å»ºå·¥å…·
å·¥å…·å¯ä»¥è®©æ¨¡å‹é€šè¿‡æˆ‘ä»¬å®šä¹‰çš„å‡½æ•°è°ƒç”¨çš„æ–¹å¼ä¸å¤–éƒ¨ç³»ç»Ÿäº¤äº’ï¼Œå·¥å…·ä¹Ÿå¯ä»¥ä¾èµ–è¿è¡Œæ—¶å’Œæ™ºèƒ½ä½“è®°å¿†ã€‚

```
from dataclasses import dataclass
from langchain.tools import tool, ToolRuntime

@tool
def get_weather_for_location(city: str) -> str:
    """Get weather for a given city."""
    return f"It's always sunny in {city}!"

@dataclass
class Context:
    """Custom runtime context schema."""
    user_id: str

@tool
def get_user_location(runtime: ToolRuntime[Context]) -> str:
    """Retrieve user information based on user ID."""
    user_id = runtime.context.user_id
    return "Florida" if user_id == "1" else "SF"
```

* é…ç½®æ¨¡å‹

æ ¹æ®ä¸‹é¢çš„ä»£ç ï¼Œé…ç½®æ¨¡å‹å‚æ•°ï¼Œæ”¹æˆè‡ªå·±çš„å‚æ•°ï¼š

```
from langchain.chat_models import init_chat_model

model = init_chat_model(
    "claude-sonnet-4-5-20250929",
    temperature=0.5,
    timeout=10,
    max_tokens=1000
)
```

* å®šä¹‰å›å¤æ ¼å¼
æˆ‘ä»¬å¯ä»¥é€šè¿‡å®šä¹‰ç»“æ„åŒ–çš„å›å¤æ ¼å¼ï¼Œæ¥ç¡®ä¿æ™ºèƒ½ä½“çš„å›å¤ç¬¦åˆæˆ‘ä»¬çš„é¢„æœŸã€‚

```
from dataclasses import dataclass

# We use a dataclass here, but Pydantic models are also supported.
@dataclass
class ResponseFormat:
    """Response schema for the agent."""
    # A punny response (always required)
    punny_response: str
    # Any interesting information about the weather if available
    weather_conditions: str | None = None
```

* æ·»åŠ è®°å¿†
ä¸ºäº†ç»´æŠ¤ä¸æ™ºèƒ½ä½“äº¤äº’å·¥ç¨‹ä¸­çš„çŠ¶æ€ï¼Œæˆ‘ä»¬å¯ä»¥æ·»åŠ å¯¹è¯è®°å¿†ï¼Œè¿™æ ·æ™ºèƒ½ä½“å°±èƒ½è®°ä½ä¹‹å‰çš„å¯¹è¯å’Œä¸Šä¸‹æ–‡ã€‚

```
from langgraph.checkpoint.memory import InMemorySaver

checkpointer = InMemorySaver()
```

* åˆ›å»ºå¹¶è¿è¡Œæ™ºèƒ½ä½“

å°†æ‰€æœ‰ç»„ä»¶ç»„è£…åˆ°æ™ºèƒ½ä½“ä¸­ï¼Œå¹¶è¿è¡Œã€‚

```
from langchain.agents.structured_output import ToolStrategy

agent = create_agent(
    model=model,
    system_prompt=SYSTEM_PROMPT,
    tools=[get_user_location, get_weather_for_location],
    context_schema=Context,
    response_format=ToolStrategy(ResponseFormat),
    checkpointer=checkpointer
)

# `thread_id` is a unique identifier for a given conversation.
config = {"configurable": {"thread_id": "1"}}

response = agent.invoke(
    {"messages": [{"role": "user", "content": "what is the weather outside?"}]},
    config=config,
    context=Context(user_id="1")
)

print(response['structured_response'])
# ResponseFormat(
#     punny_response="Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!",
#     weather_conditions="It's always sunny in Florida!"
# )


# Note that we can continue the conversation using the same `thread_id`.
response = agent.invoke(
    {"messages": [{"role": "user", "content": "thank you!"}]},
    config=config,
    context=Context(user_id="1")
)

print(response['structured_response'])
# ResponseFormat(
#     punny_response="You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!",
#     weather_conditions=None
# )
```

## LangChain ä¸­çš„â€œé“¾â€
é“¾å¼è°ƒç”¨ä½äºLangChainä¸‰å±‚æ ¸å¿ƒæ¶æ„ä¸­çš„ä¸­é—´å±‚â€”â€”å·¥ä½œæµAPIæŠ½è±¡å±‚ã€‚Chainç¿»è¯‘æˆä¸­æ–‡å°±æ˜¯â€œé“¾â€ï¼Œæˆ‘ä»¬å°†å¤§æ¨¡å‹ã€ç›¸å…³å·¥å…·ç­‰ä½œä¸ºç»„ä»¶ï¼Œé“¾å°±æ˜¯è´Ÿè´£å°†è¿™äº›ç»„ä»¶æŒ‰ç…§æŸä¸€ç§é€»è¾‘ï¼Œé¡ºåºç»„åˆæˆä¸€ä¸ªæµæ°´çº¿çš„æ–¹å¼ã€‚æ¯”å¦‚æˆ‘ä»¬è¦æ„å»ºä¸€ä¸ªç®€å•çš„é—®ç­”é“¾ï¼Œå°±éœ€è¦æŠŠå¤§æ¨¡å‹ç»„ä»¶å’Œæ ‡å‡†è¾“å‡ºç»„ä»¶ç”¨é“¾ä¸²è”èµ·æ¥ã€‚

### LangChain é“¾ä»£ç å®ç°
1. ç®€å•é“¾

æ­å»ºä¸€ä¸ªç®€å•é“¾ï¼Œå°†æ¨¡å‹â€œè¾“å‡ºç»“æœâ€è¿‡æ»¤ä¸ºä¸€ä¸ªçº¯å­—ç¬¦ä¸²æ ¼å¼:

```
from langchain.chat_models import init_chat_model
from langchain_core.output_parsers import StrOutputParser # å¯¼å…¥æ ‡å‡†è¾“å‡ºç»„ä»¶

model = init_chat_model(
    model="doubao-1-5-pro-32k-250115",
    model_provider="openai",
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    api_key=os.environ["ARK_OPENAI_API_KEY"], #ä½ æ³¨å†Œçš„ç«å±±å¼•æ“api_key
)

# æ­å»ºé“¾æ¡ï¼ŒæŠŠmodelå’Œå­—ç¬¦ä¸²è¾“å‡ºè§£æå™¨ç»„ä»¶è¿æ¥åœ¨ä¸€èµ·
basic_qa_chain = model | StrOutputParser()

# æŸ¥çœ‹è¾“å‡ºç»“æœ
question = "ä½ å¥½ï¼Œè¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
result = basic_qa_chain.invoke(question)

print(result)

```

è¿è¡Œä¸Šé¢çš„ä»£ç ï¼Œå¯ä»¥çœ‹åˆ°æ­¤æ—¶çš„resultä¸å†æ˜¯åŒ…å«æ¨¡å‹å„ç§è°ƒç”¨ä¿¡æ¯çš„AIMessageå¯¹è±¡ï¼Œè€Œæ˜¯çº¯ç²¹çš„æ¨¡å‹å“åº”çš„å­—ç¬¦ä¸²ç»“æœã€‚


2. æç¤ºè¯æ¨¡æ¿åˆ›å»ºé“¾
é“¾æµç¨‹å¢åŠ ä¸€ä¸ªæç¤ºè¯æ¨¡æ¿ï¼Œå¯ä»¥å€ŸåŠ©ChatPromptTemplateéå¸¸ä¾¿æ·çš„å°†ä¸€ä¸ªæç¤ºè¯æ¨¡æ¿æ‰“é€ ä¸ºç»„ä»¶ï¼ŒåŒæ ·ä»¥é“¾çš„å½¢å¼åŠ å…¥å½“å‰æµç¨‹ä¸­

```
import os
from langchain.chat_models import init_chat_model
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

model = init_chat_model(
    model="doubao-1-5-pro-32k-250115",
    model_provider="openai",
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    api_key=os.environ["ARK_OPENAI_API_KEY"], #ä½ æ³¨å†Œçš„ç«å±±å¼•æ“api_key
)

prompt_template = ChatPromptTemplate([
    ("system", "ä½ æ˜¯ä¸€ä¸ªä¹æ„åŠ©äººçš„åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç»™å‡ºå›ç­”"),
    ("user", "è¿™æ˜¯ç”¨æˆ·çš„é—®é¢˜ï¼š {topic}ï¼Œ è¯·ç”¨ yes æˆ– no æ¥å›ç­”")
])

# ç›´æ¥ä½¿ç”¨æ¨¡å‹ + è¾“å‡ºè§£æå™¨
bool_qa_chain = prompt_template | model | StrOutputParser()
# æµ‹è¯•
question = "è¯·é—® 1 + 1 æ˜¯å¦ å¤§äº 2ï¼Ÿ"
result = bool_qa_chain.invoke({'topic':question})
print(result)

```

å€ŸåŠ©æç¤ºè¯æ¨¡æ¿å³å¯å®ç°ç›¸åº”çš„ç»“æ„åŒ–è¾“å‡ºã€‚


3. ç»“æ„åŒ–è§£æå™¨

LangChainä¸­ä¸€ä¸ªåŸºç¡€çš„é“¾ä¸€èˆ¬ç”±å¦‚ä¸‹ä¸‰éƒ¨åˆ†æ„æˆï¼Œåˆ†åˆ«æ˜¯æç¤ºè¯æ¨¡æ¿ã€å¤§æ¨¡å‹å’Œç»“æ„åŒ–è§£æå™¨ã€‚æ™ºèƒ½ä½“å¼€å‘äººå‘˜é€šè¿‡æç¤ºè¯è®©å¤§æ¨¡å‹è¾“å‡ºç»“æ„åŒ–çš„å­—ç¬¦ä¸²ï¼Œç„¶åé€šè¿‡ç»“æ„åŒ–è§£æå™¨å°†å­—ç¬¦ä¸²è§£æä¸ºæŒ‡å®šå¯¹è±¡ã€‚æµç¨‹ä¸º:


```mermaid
graph TD
    A[ç”¨æˆ·è¾“å…¥] --> B[PromptTemplate <br/>æç¤ºè¯æ¨¡æ¿]
    B --> C[ChatModel <br/>å¤§æ¨¡å‹]
    C --> D[OutputParser <br/>ç»“æ„åŒ–è§£æ]
    D --> E[ç»“æ„åŒ–ç»“æœ]
```


LangChainä¸­å¸¸ç”¨çš„æ ¸å¿ƒç»“æ„åŒ–è§£æå™¨åŠŸèƒ½å¦‚ä¸‹:


| è§£æå™¨åç§° | åŠŸèƒ½æè¿° | ç±»å‹ |
|-----------|----------|------|
| BooleanOutputParser | å°† LLM è¾“å‡ºè§£æä¸ºå¸ƒå°”å€¼ | åŸºç¡€ç±»å‹è§£æ |
| DatetimeOutputParser | å°† LLM è¾“å‡ºè§£æä¸ºæ—¥æœŸæ—¶é—´ | åŸºç¡€ç±»å‹è§£æ |
| EnumOutputParser | è§£æè¾“å‡ºä¸ºé¢„å®šä¹‰æšä¸¾å€¼ä¹‹ä¸€ | åŸºç¡€ç±»å‹è§£æ |
| RegexParser | ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£æ LLM è¾“å‡º | æ¨¡å¼åŒ¹é…è§£æ |
| RegexDictParser | ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å°†è¾“å‡ºè§£æä¸ºå­—å…¸ | æ¨¡å¼åŒ¹é…è§£æ |
| StructuredOutputParser | å°† LLM è¾“å‡ºè§£æä¸ºç»“æ„åŒ–æ ¼å¼ | ç»“æ„åŒ–è§£æ |
| YamlOutputParser | ä½¿ç”¨ Pydantic æ¨¡å‹è§£æ YAML è¾“å‡º | ç»“æ„åŒ–è§£æ |
| PandasDataFrameOutputParser | ä½¿ç”¨ Pandas DataFrame æ ¼å¼è§£æè¾“å‡º | æ•°æ®å¤„ç†è§£æ |
| CombiningOutputParser | å°†å¤šä¸ªè¾“å‡ºè§£æå™¨ç»„åˆä¸ºä¸€ä¸ª | ç»„åˆè§£æå™¨ |
| OutputFixingParser | åŒ…è£…è§£æå™¨å¹¶å°è¯•ä¿®å¤è§£æé”™è¯¯ | é”™è¯¯å¤„ç†è§£æ |
| RetryOutputParser | åŒ…è£…è§£æå™¨å¹¶å°è¯•ä¿®å¤è§£æé”™è¯¯ | é”™è¯¯å¤„ç†è§£æ |
| RetryWithErrorOutputParser | åŒ…è£…è§£æå™¨å¹¶å°è¯•ä¿®å¤è§£æé”™è¯¯ | é”™è¯¯å¤„ç†è§£æ |
| ResponseSchema | ç»“æ„åŒ–è¾“å‡ºè§£æå™¨çš„å“åº”æ¨¡å¼ | è¾…åŠ©ç±» |

4. å¤æ‚é“¾æ„é€ 

æˆ‘ä»¬ä»¥ä¸€ä¸ªâ€ä»ç¾é£Ÿèœè°±åç§°ç”Ÿæˆåˆ¶ä½œæ­¥éª¤ï¼Œå¹¶ä»ä¸­æå–æ ¸å¿ƒé£Ÿæå’Œçƒ¹é¥ªæ—¶é•¿â€ çš„æ¡ˆä¾‹æ¥è®²è§£ï¼Œè¿™ä¸ªæ¡ˆä¾‹åŒæ ·åŒ…å«äº†â€œæ–‡æœ¬ç”Ÿæˆâ€å’Œâ€œç»“æ„åŒ–ä¿¡æ¯æå–â€ä¸¤ä¸ªç¯èŠ‚ã€‚

```
import os
from langchain.chat_models import init_chat_model
from langchain_core.prompts import PromptTemplate
# è¿™ä¸ªåŒ…åœ¨æ‰€æœ‰ 0.1+ ç‰ˆæœ¬ä¸­éƒ½éå¸¸ç¨³å®š
from langchain_core.output_parsers import JsonOutputParser

# 1. åˆå§‹åŒ–æ¨¡å‹
model = init_chat_model(
    model="doubao-1-5-pro-32k-250115",
    model_provider="openai",
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    api_key=os.environ.get("ARK_OPENAI_API_KEY"),
)

# 2. åˆå§‹åŒ– JsonOutputParser (ä¸éœ€è¦å®šä¹‰å¤æ‚çš„ ResponseSchema)
parser = JsonOutputParser()

# 3. ç¼–å†™é“¾
# ç¬¬ä¸€æ­¥ç”Ÿæˆæ–‡æœ¬
gen_prompt = PromptTemplate.from_template("è¯·æ ¹æ®èœåç¼–å†™ä¸€ä¸ªç®€çŸ­çš„åˆ¶ä½œæ­¥éª¤, åŒ…æ‹¬åˆ¶ä½œéš¾åº¦ä¸è€—æ—¶ï¼š{dish_name}")
# ç¬¬äºŒæ­¥æå–å¹¶å¼ºåˆ¶è¦æ±‚ JSON
extract_prompt = PromptTemplate.from_template(
    "ä»ä»¥ä¸‹å†…å®¹ä¸­æå–ä¿¡æ¯ï¼šé£Ÿææ¸…å•(ingredients)ã€çƒ¹é¥ªæ—¶é•¿(time)ã€éš¾åº¦(difficulty)ã€‚\n"
    "å¿…é¡»ä»¥ JSON æ ¼å¼è¿”å›ã€‚\n"
    "å†…å®¹ï¼š{recipe}"
)

# 4. ç»„åˆé“¾
full_chain = (
    {"recipe": gen_prompt | model | (lambda x: x.content)}
    | extract_prompt
    | model
    | parser  # ç›´æ¥è§£æ JSON
)

# æ‰§è¡Œ
try:
    result = full_chain.invoke({"dish_name": "åœ°ä¸‰é²œ"})
    print(result)
except Exception as e:
    print(f"ä»ç„¶æŠ¥é”™: {e}")
```

### è‡ªå®šä¹‰Langchainç»„ä»¶
Langchain æä¾›äº†å¼€å‘è€…è‡ªå®šä¹‰å¯è¿è¡ŒèŠ‚ç‚¹çš„åŠŸèƒ½.å¦‚æœæˆ‘ä»¬æƒ³åœ¨é“¾ä¸­è®¾ç½®è°ƒè¯•ç»„ä»¶è¯¥å¦‚ä½•ç¼–å†™ä»£ç ?è¿™å°±éœ€è¦ç”¨åˆ°LangChainçš„Runnableç»„ä»¶äº†ã€‚ åœ¨ä¸Šè¿°å¤åˆé“¾ä»£ç ä¸­æ·»åŠ :
```
from langchain_core.runnables import RunnableLambda

def debug_print(x):
    print('ä¸­é—´ç»“æœï¼š', x)
    return x

debug_node = RunnableLambda(debug_print)

# ç»„åˆæˆä¸€ä¸ªå¤åˆ Chain
full_chain = (
    {"recipe": gen_prompt | debug_node | model | (lambda x: x.content)} | debug_node
    | extract_prompt
    | model | debug_node
    | parser  # ç›´æ¥è§£æ JSON
)

# è°ƒç”¨å¤åˆé“¾
result = full_chain.invoke({"title": "åœ°ä¸‰é²œ"})
print(result)

```

è¿è¡Œä¸Šé¢çš„ä»£ç å¯ä»¥çœ‹åˆ°æ¯ä¸€æ­¥éƒ½ä¼šæœ‰ä¸­é—´ç»“æœè¾“å‡ºã€‚


RunnableLambdaå°†pythonå‡½æ•°è½¬æ¢ä¸ºå¯è¿è¡ŒèŠ‚ç‚¹ã€‚è½¬åŒ–åçš„èŠ‚ç‚¹å¯ä»¥åƒä»»ä½•å…¶å®ƒRunnableä¸€æ ·ç»„åˆå¹¶ä¸LangChainé“¾æ— ç¼é›†æˆã€‚ï¼ˆç‰¹åˆ«æ³¨æ„: RunnableLambdaé€‚åˆéæµå¼è¾“å‡ºï¼Œå¦‚æœè¦æµå¼è¾“å‡ºè¯·ä½¿ç”¨RunnableGenerator python.langchain.com/api_referenâ€¦ï¼‰ã€‚

## LCELç®€è¿°
ä»£ç ä¸­|ç¬¦å·è¢«æˆ‘ä»¬å¹¿æ³›ä½¿ç”¨ï¼ŒPythonæ²¡æœ‰è¿™ç§è¯­æ³•ï¼Œä¸ºä»€ä¹ˆè¿™é‡Œå¯ä»¥æŠŠå„ä¸ªç»„ä»¶ä¸²èµ·æ¥ã€‚

å…¶å®è¿™æ˜¯LangChainä¸“é—¨ä¸ºç°ä»£å¤§è¯­è¨€æ¨¡å‹åº”ç”¨å¼€å‘çš„ä¸€ç§å…¨æ–°è¡¨è¾¾èŒƒå¼ï¼Œè¢«ç§°ä¸ºLCELï¼ˆLangChain Expression Languageï¼‰ ã€‚å®ƒä¸ä»…ç®€åŒ–äº†æ¨¡å‹äº¤äº’çš„ç¼–æ’è¿‡ç¨‹ï¼Œè¿˜å¢å¼ºäº†ç»„åˆçš„çµæ´»æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚

###  LCELçš„å®šä¹‰
LCELï¼Œå…¨ç§°ä¸ºLangChain Expression Languageï¼Œæ˜¯ä¸€ç§ä¸“ä¸º LangChain æ¡†æ¶è®¾è®¡çš„è¡¨è¾¾è¯­è¨€ã€‚å®ƒé€šè¿‡ä¸€ç§é“¾å¼ç»„åˆçš„æ–¹å¼ï¼Œå…è®¸å¼€å‘è€…ä½¿ç”¨æ¸…æ™°ã€å£°æ˜å¼çš„è¯­æ³•æ¥æ„å»ºè¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨æµç¨‹ã€‚
ç®€å•æ¥è¯´ï¼ŒLCEL æ˜¯ä¸€ç§â€œå‡½æ•°å¼ç®¡é“é£æ ¼â€çš„ç»„ä»¶ç»„åˆæœºåˆ¶ï¼Œç”¨äºè¿æ¥å„ç§å¯æ‰§è¡Œå•å…ƒï¼ˆRunnableï¼‰ã€‚è¿™äº›å•å…ƒåŒ…æ‹¬æç¤ºæ¨¡æ¿ã€è¯­è¨€æ¨¡å‹ã€è¾“å‡ºè§£æå™¨ã€å·¥å…·å‡½æ•°ç­‰ã€‚

### LCELçš„è®¾è®¡ç›®çš„
LCEL çš„è®¾è®¡åˆè¡·åœ¨äºï¼š

*æ¨¡å—åŒ–æ„å»º*ï¼šå°†æ¨¡å‹è°ƒç”¨æµç¨‹æ‹†è§£ä¸ºç‹¬ç«‹ã€å¯é‡ç”¨çš„ç»„ä»¶ã€‚
*é€»è¾‘å¯è§†åŒ–*ï¼šé€šè¿‡è¯­æ³•ç¬¦å·ï¼ˆå¦‚ç®¡é“ç¬¦ |ï¼‰å‘ˆç°å‡ºæ˜ç¡®çš„æ•°æ®æµè·¯å¾„ã€‚
*ç»Ÿä¸€è¿è¡Œæ¥å£*ï¼šæ‰€æœ‰ LCEL ç»„ä»¶éƒ½å®ç°äº† .invoke()ã€.stream()ã€.batch() ç­‰æ ‡å‡†æ–¹æ³•ï¼Œä¾¿äºåœ¨åŒæ­¥ã€å¼‚æ­¥æˆ–æ‰¹å¤„ç†ç¯å¢ƒä¸‹è°ƒç”¨ã€‚
*è„±ç¦»æ¡†æ¶é™åˆ¶*ï¼šç›¸æ¯”ä¼ ç»Ÿçš„ Chain ç±»å’Œ Agent æ¶æ„ï¼ŒLCEL æ›´è½»é‡ã€æ›´å…·è¡¨è¾¾åŠ›ï¼Œå‡å°‘ä¾èµ–çš„â€œé»‘ç›’â€é€»è¾‘ã€‚

### LCELçš„æ ¸å¿ƒç»„æˆ
LCELçš„æ ¸å¿ƒç»„æˆæœ‰å¦‚ä¸‹ä¸‰ç‚¹:

1. Runnable æ¥å£

LCEL çš„ä¸€åˆ‡åŸºç¡€å•å…ƒéƒ½æ˜¯ Runnable å¯¹è±¡ï¼Œå®ƒæ˜¯ä¸€ç§ç»Ÿä¸€çš„å¯è°ƒç”¨æ¥å£ï¼Œæ”¯æŒå¦‚ä¸‹å½¢å¼ï¼š
.invoke(input)ï¼šåŒæ­¥è°ƒç”¨
.stream(input)ï¼šæµå¼ç”Ÿæˆ
.batch(inputs)ï¼šæ‰¹é‡æ‰§è¡Œ

2. ç®¡é“è¿ç®—ç¬¦ |
è¿™æ˜¯ LCEL æœ€å…·ç‰¹è‰²çš„è¯­æ³•ç¬¦å·ã€‚å¤šä¸ª Runnable å¯¹è±¡(ä¹Ÿå°±æ˜¯æˆ‘ä»¬è¯´çš„ç»„ä»¶)å¯ä»¥é€šè¿‡ | ä¸²è”èµ·æ¥ï¼Œå½¢æˆæ¸…æ™°çš„æ•°æ®å¤„ç†é“¾ã€‚ä¾‹å¦‚ï¼š
```
prompt | model | parser
```

è¡¨ç¤ºæ•°æ®å°†ä¾æ¬¡ä¼ å…¥æç¤ºæ¨¡æ¿ã€æ¨¡å‹å’Œè¾“å‡ºè§£æå™¨ï¼Œæœ€ç»ˆè¾“å‡ºç»“æ„åŒ–ç»“æœã€‚

3. PromptTemplate ä¸ OutputParser
LCEL å¼ºè°ƒç»„ä»¶ä¹‹é—´çš„èŒè´£æ˜ç¡®ï¼ŒPrompt åªè´Ÿè´£æ¨¡æ¿åŒ–è¾“å…¥ï¼ŒParser åªè´Ÿè´£æ ¼å¼åŒ–è¾“å‡ºï¼ŒModel åªè´Ÿè´£æ¨ç†ã€‚

## LangChainè®°å¿†å­˜å‚¨

### LangChainå•è½®ä¼šè¯

ç¼–å†™LangChainå•è½®å¯¹è¯çš„åŸºæœ¬æµç¨‹å¦‚ä¸‹:

1. å¯¼å…¥ç›¸å…³ä¾èµ–åŒ…å¹¶åˆå§‹åŒ–æç¤ºè¯ChatPromptTemplate
2. è°ƒç”¨ç»Ÿä¸€æ¥å£init_chat_modelåˆå§‹åŒ–å¤§æ¨¡å‹ç»„ä»¶
3. ä½¿ç”¨LCELè¯­æ³•å°†å¤§æ¨¡å‹ç»„ä»¶å’Œè¾“å‡ºè§£æå™¨ç»„ä»¶ç›¸è¿æ¥ï¼Œå½¢æˆâ€œé“¾â€
4. æ‰§è¡Œâ€œé“¾â€å¹¶è¾“å‡ºç»“æœ

```
from langchain_core.output_parsers import StrOutputParser
from langchain.chat_models import init_chat_model
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import init_chat_model


chatbot_prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ å«å°æ™ºï¼Œæ˜¯äººå·¥æ™ºèƒ½ä¸“å®¶ã€‚"),
    ("user", "{input}")
])

# ä½¿ç”¨æ¨¡å‹
model = init_chat_model(
    model="Qwen/Qwen3-8B",
    model_provider="openai",
    base_url="",
    api_key="ä½ æ³¨å†Œçš„api key",
)

# ç›´æ¥ä½¿ç”¨æ¨¡å‹ + è¾“å‡ºè§£æå™¨
basic_qa_chain = chatbot_prompt | model | StrOutputParser()

# æµ‹è¯•
question = "ä½ å¥½ï¼Œè¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
result = basic_qa_chain.invoke(question)
print(result)

```

### LangChainå¤šè½®è®°å¿†
è¦æŠŠå•è½®å¯¹è¯ä¿®æ”¹ä¸ºå¤šè½®å¯¹è¯æˆ‘ä»¬åº”è¯¥æ€ä¹ˆåšå‘¢ï¼Ÿé€»è¾‘å…¶å®å¾ˆç®€å•ï¼Œåœ¨LangChainä¸­æˆ‘ä»¬å¯ä»¥é€šè¿‡äººå·¥æ‹¼æ¥æ¶ˆæ¯é˜Ÿåˆ—æ¥ä¸ºæ¯æ¬¡æ¨¡å‹è°ƒç”¨è®¾ç½®å¤šè½®å¯¹è¯è®°å¿†ã€‚éœ€è¦è¿›è¡Œå¦‚ä¸‹æ­¥éª¤ï¼š

1. æ„å»ºæç¤ºè¯ç»„ä»¶ChatPromptTemplateæ—¶ï¼Œé€šè¿‡å ä½ç¬¦MessagePlaceholderå®šä¹‰ä¸€ä¸ªæ¶ˆæ¯åˆ—è¡¨, å…³é”®ä»£ç ä¸ºï¼š
```
prompt = ChatPromptTemplate.from_messages([ 
    SystemMessage(content="ä½ å«å°æ™ºï¼Œæ˜¯äººå·¥æ™ºèƒ½ä¸“å®¶ã€‚"),  
    MessagesPlaceholder(variable_name="messages"), 
])
```

2. åœ¨å¤šè½®å¯¹è¯ä¸­ä¸æ–­çš„å‘messageåˆ—è¡¨ä¸­è¿½åŠ æ¶ˆæ¯ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™å ä½ç¬¦ï¼Œå¤§æ¨¡å‹ç»„ä»¶æ¥æ”¶åˆ°åˆ—è¡¨ä¿¡æ¯åä¼šè‡ªåŠ¨å…³è”å†å²æ¶ˆæ¯å¹¶å›å¤å†…å®¹, å…³é”®ä»£ç ä¸º:

```
messages_list.append(HumanMessage(content=user_query)) 

assistant_reply = chain.invoke({"messages": messages_list}) 

print("å°æ™ºï¼š", assistant_reply)
```

å®Œæ•´çš„å¤šè½®å¯¹è¯ä»£ç å¦‚ä¸‹:

```
import os
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chat_models import init_chat_model
from langchain_core.output_parsers import StrOutputParser

# ä½¿ç”¨ ç¡…åŸºæµåŠ¨ æ¨¡å‹
model = init_chat_model(
    model="doubao-1-5-pro-32k-250115",
    model_provider="openai",
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    api_key=os.environ.get("ARK_OPENAI_API_KEY"),
)

parser = StrOutputParser()

prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content="ä½ å«å°æ™ºï¼Œæ˜¯äººå·¥æ™ºèƒ½ä¸“å®¶ã€‚"),
    MessagesPlaceholder(variable_name="messages"),
])

chain = prompt | model | parser

messages_list = []  # åˆå§‹åŒ–å†å²
print("ğŸ”¹ è¾“å…¥ exit ç»“æŸå¯¹è¯")
while True:
    user_query = input("ä½ ï¼š")
    if user_query.lower() in {"exit", "quit"}:
        break
    messages_list.append(HumanMessage(content=user_query))
    assistant_reply = chain.invoke({"messages": messages_list})
    print("å°æ™ºï¼š", assistant_reply)
    messages_list.append(AIMessage(content=assistant_reply))
    messages_list = messages_list[-50:]
```

### æµå¼æ‰“å°
LangChainæä¾›äº†ä¸€ä¸ªstreamæ–¹æ³•ï¼Œå¯ä»¥å®ç°æµå¼è¾“å‡ºï¼Œåªéœ€è¦åœ¨è°ƒç”¨æ¨¡å‹å›ç­”æ—¶å°†invokeæ–¹æ³•æ›¿æ¢ä¸ºstreamå³å¯ã€‚stream()æ˜¯åŒæ­¥æ–¹æ³•,ä½¿ç”¨forå¾ªç¯æ¥å—è¿”å›çš„chunkå—ã€‚å¦‚æœå¼‚æ­¥è°ƒç”¨ï¼Œéœ€è¦ä½¿ç”¨astream(),ç„¶åä½¿ç”¨async forå¼‚æ­¥forå¾ªç¯è·å–æ¨¡å‹è¾“å‡ºã€‚

```
#  è°ƒç”¨æ¨¡å‹
assistant_reply=''
print('å°æ™º:', end=' ')
for chunk in chain.stream({"messages": messages_list}):
    assistant_reply+=chunk
    print(chunk, end="", flush=True)
print()

# è¿½åŠ  AI å›å¤
messages_list.append(AIMessage(content=assistant_reply))

```


## Langchain æ¥å…¥å·¥å…·åŸºæœ¬æµç¨‹
### LangChainæ¥å…¥å†…ç½®å·¥å…·
LangChainç”Ÿæ€ä»å»ºç«‹èµ·å°±å†…ç½®é›†æˆäº†éå¸¸å¤šçš„å®ç”¨å·¥å…·ï¼Œå¼€å‘è€…å¯ä»¥å¿«é€Ÿè°ƒç”¨è¿™äº›å·¥å…·å®Œæˆæ›´åŠ å¤æ‚å·¥ä½œæµçš„å¼€å‘ã€‚å¯è®¿é—®å®˜æ–¹æ–‡æ¡£[Tools and toolkits](https://docs.langchain.com/oss/python/integrations/tools)æŸ¥çœ‹LangChainå†…ç½®å·¥å…·åˆ—è¡¨ã€‚LangChainå†…ç½®å·¥å…·å¯åˆ†ä¸ºSearchåœ¨çº¿æœç´¢å·¥å…·ã€Code Interpreterä»£ç è§£é‡Šå™¨å·¥å…·ã€Productivitè‡ªåŠ¨åŒ–å·¥å…·ã€WebBrowsingæµè§ˆå™¨è‡ªåŠ¨åŒ–å·¥å…·ã€Databaseæ•°æ®åº“å·¥å…·ç­‰å¤šç§ç±»åˆ«ã€‚

æ¥ä¸‹æ¥çš„ä¾‹å­ä½¿ç”¨SQLDatabase Toolkitæ¥å…¥æ•°æ®åº“å·¥å…·ã€‚

SQLDatabaseToolkit ä¸­çš„å·¥å…·æ—¨åœ¨ä¸ SQL æ•°æ®åº“è¿›è¡Œäº¤äº’ã€‚
ä¸€ä¸ªå¸¸è§çš„åº”ç”¨åœºæ™¯æ˜¯ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåˆ©ç”¨å…³ç³»å‹æ•°æ®åº“ä¸­çš„æ•°æ®æ¥å›ç­”é—®é¢˜ï¼Œç”šè‡³å¯èƒ½ä»¥è¿­ä»£çš„æ–¹å¼è¿›è¡Œæ•°æ®æŸ¥è¯¢å’Œåˆ†æã€‚

### è®¾ç½®
è¦å¯ç”¨å•ä¸ªå·¥å…·çš„è‡ªåŠ¨è¿½è¸ªåŠŸèƒ½ï¼Œè®¾ç½®LangSmith API å¯†é’¥ï¼š
```
os.environ["LANGSMITH_API_KEY"] = getpass.getpass("Enter your LangSmith API key: ")
os.environ["LANGSMITH_TRACING"] = "true"
```

### å®‰è£…
å·¥å…·åœ¨langchain-communityåŒ…é‡Œï¼Œéœ€è¦å®‰è£…:
```
pip install -qU  langchain-community
```
ä»¥åŠå®‰è£…å…¶ä»–å¿…è¦çš„ä¾èµ–é¡¹ï¼Œä¾‹å¦‚SQLAlchemyï¼š
```
pip install -qU sqlalchemy requests
```

ä¸‹é¢æˆ‘ä»¬å°†ä½¿ç”¨ requests åº“æ‹‰å– .sql æ–‡ä»¶å¹¶åˆ›å»ºä¸€ä¸ªå†…å­˜ä¸­çš„ SQLite æ•°æ®åº“ã€‚è¯·æ³¨æ„ï¼Œè¿™ç§æ–¹æ³•è™½ç„¶è½»é‡çº§ï¼Œä½†å…·æœ‰ä¸´æ—¶æ€§ä¸”ä¸æ˜¯çº¿ç¨‹å®‰å…¨çš„ã€‚ä¹Ÿå¯ä»¥æŒ‰ç…§è¯´æ˜å°†æ–‡ä»¶æœ¬åœ°ä¿å­˜ä¸º Chinook.dbï¼Œå¹¶é€šè¿‡ db = SQLDatabase.from_uri("sqlite:///Chinook.db") å®ä¾‹åŒ–æ•°æ®åº“ã€‚

```
import sqlite3

import requests
from langchain_community.utilities.sql_database import SQLDatabase
from sqlalchemy import create_engine
from sqlalchemy.pool import StaticPool


def get_engine_for_chinook_db():
    """Pull sql file, populate in-memory database, and create engine."""
    url = "https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql"
    response = requests.get(url)
    sql_script = response.text

    connection = sqlite3.connect(":memory:", check_same_thread=False)
    connection.executescript(sql_script)
    return create_engine(
        "sqlite://",
        creator=lambda: connection,
        poolclass=StaticPool,
        connect_args={"check_same_thread": False},
    )


engine = get_engine_for_chinook_db()

db = SQLDatabase(engine)
```

åˆ›å»ºä¸€ä¸ªå¤§æ¨¡å‹ç¤ºä¾‹ï¼š
```
llm = init_chat_model(
    model="doubao-1-5-pro-32k-250115",
    model_provider="openai",
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    api_key=os.environ.get("ARK_OPENAI_API_KEY"),
)
```

å®ä¾‹åŒ–ä¸€ä¸ªå·¥å…·é›†:
```
from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit

toolkit = SQLDatabaseToolkit(db=db, llm=llm)
```

### åœ¨æ™ºèƒ½ä½“ä¸­ä½¿ç”¨å·¥å…·é›†
æ¥ä¸‹æ¥æˆ‘ä»¬ä¸ºä¸€ä¸ªç®€å•çš„é—®ç­”ä»£ç†é…å¤‡æˆ‘ä»¬å·¥å…·åŒ…ä¸­çš„å·¥å…·ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è·å–ä¸€ä¸ªç›¸å…³çš„æç¤ºè¯å¹¶ç”¨å…¶æ‰€éœ€å‚æ•°å¡«å……å®ƒï¼š
```
from langchain_classic import hub

prompt_template = hub.pull("langchain-ai/sql-agent-system-prompt")

assert len(prompt_template.messages) == 1
print(prompt_template.input_variables)

system_message = prompt_template.format(dialect="SQLite", top_k=5)
```

å®ä¾‹åŒ–ä¸€ä¸ªæ™ºèƒ½ä½“ï¼š
```
from langchain.agents import create_agent

agent = create_agent(llm, toolkit.get_tools(), system_prompt=system_message)
```

æœ€åï¼Œè¿›è¡ŒæŸ¥è¯¢ï¼š
```
example_query = "Which country's customers spent the most?"

events = agent.stream(
    {"messages": [("user", example_query)]},
    stream_mode="values",
)
for event in events:
    event["messages"][-1].pretty_print()
```

è¾“å‡ºå†…å®¹å¦‚ä¸‹ï¼š

```
...
================================== Ai Message ==================================

The top 5 countries whose customers spent the most are as follows:
| Country | TotalSpent |
| ---- | ---- |
| USA | 523.06 |
| Canada | 303.96 |
| France | 195.1 |
| Brazil | 190.1 |
| Germany | 156.48 |

So, the customers from the USA spent the most.
```

### Langchainæ¥å…¥è‡ªå®šä¹‰å·¥å…·

é™¤äº†ä½¿ç”¨LangChainçš„å†…éƒ¨å·¥å…·ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥è‡ªè¡Œåˆ›å»ºå¤–éƒ¨å‡½æ•°å¹¶å°†å…¶å°è£…ä¸ºä¸€ä¸ªLangChain"é“¾"å¯è°ƒç”¨çš„toolç»„ä»¶ã€‚å®ç°ä¸€ä¸ªè·å–å¤©æ°”çš„æ™ºèƒ½ä½“å·¥å…·ï¼Œå…·ä½“çš„æ­¥éª¤å¦‚ä¸‹ï¼š

1. å¿ƒçŸ¥å¤©æ°”æ³¨å†ŒåŠAPI keyè·å–

æ‰“å¼€å¿ƒçŸ¥å¤©æ°”çš„[å®˜ç½‘](https://www.seniverse.com/console)ï¼Œæ³¨å†Œç™»å½•å¹¶ç‚¹å‡»æ§åˆ¶å°:

åˆ›å»ºå¥½å¯†é’¥ä¹‹åå¯ä»¥åœ¨åç»­ä½¿ç”¨ã€‚

2. ç¼–å†™éªŒè¯API keyä»£ç  
åˆ©ç”¨python requestsåº“è°ƒç”¨APIè·å¾—å¤©æ°”æƒ…å†µï¼ˆå…è´¹ç‰ˆçš„åªèƒ½å¾—åˆ°å¤©æ°”ç°è±¡ã€å¤©æ°”ç°è±¡ä»£ç å’Œæ°”æ¸© 3é¡¹æ•°æ®ï¼‰
```
import requests

url = "https://api.seniverse.com/v3/weather/now.json"

params = {
    "key": "",  # å¡«å†™ä½ çš„ç§é’¥
    "location": "åŒ—äº¬",  # ä½ è¦æŸ¥è¯¢çš„åœ°åŒºå¯ä»¥ç”¨ä»£å·ï¼Œæ‹¼éŸ³æˆ–è€…æ±‰å­—ï¼Œæ–‡æ¡£åœ¨å®˜æ–¹ä¸‹è½½ï¼Œè¿™é‡Œä¸¾ä¾‹åŒ—äº¬
    "language": "zh-Hans",  # ä¸­æ–‡ç®€ä½“
    "unit": "c",  # è·å–æ°”æ¸©
}

response = requests.get(url, params=params)  # å‘é€getè¯·æ±‚
temperature = response.json()  # æ¥å—æ¶ˆæ¯ä¸­çš„jsonéƒ¨åˆ†
print(temperature['results'][0]['now'])  # è¾“å‡ºæ¥æ”¶åˆ°çš„æ¶ˆæ¯è¿›è¡ŒæŸ¥çœ‹

```

3. ä¸ºäº†è®©å¤§æ¨¡å‹èƒ½å¤Ÿè°ƒç”¨å¤©æ°”å·¥å…·ï¼Œéœ€è¦å°†è°ƒç”¨å¤©æ°”APIå°è£…æˆå‡½æ•°.

```
import requests

def get_weather(loc):
    url = "https://api.seniverse.com/v3/weather/now.json"
    params = {
        "key": "", #å¡«å†™ä½ çš„ç§é’¥
        "location": loc,
        "language": "zh-Hans",
        "unit": "c",
    }
    response = requests.get(url, params=params)
    temperature = response.json()
    return temperature['results'][0]['now']

```

4.  è®©å¤§æ¨¡å‹ç†è§£å‡½æ•°, æ„é€ Fuction Call
å‡†å¤‡å¥½å¤–éƒ¨å‡½æ•°ä¹‹åï¼Œéå¸¸é‡è¦çš„ä¸€æ­¥æ˜¯å°†å¤–éƒ¨å‡½æ•°çš„ä¿¡æ¯ä»¥æŸç§å½¢å¼ä¼ è¾“ç»™å¤§æ¨¡å‹ï¼Œè®©å¤§æ¨¡å‹ç†è§£å‡½æ•°çš„ä½œç”¨ã€‚å¤§æ¨¡å‹éœ€è¦ç‰¹å®šçš„å­—å…¸æ ¼å¼å¯¹å‡½æ•°è¿›è¡Œå®Œæ•´æè¿°, å­—å…¸æè¿°åŒ…æ‹¬:
- name:å‡½æ•°åç§°å­—ç¬¦ä¸²
- description: æè¿°å‡½æ•°åŠŸèƒ½çš„å­—ç¬¦ä¸²ï¼Œå¤§æ¨¡å‹é€‰æ‹©å‡½æ•°çš„æ ¸å¿ƒä¾æ®
- parameters: å‡½æ•°å‚æ•°, è¦æ±‚éµç…§JSON Schemaæ ¼å¼è¾“å…¥ï¼ŒJSON Schemaæ ¼å¼è¯·å‚ç…§[JSON Schemaæ ¼å¼è¯¦è§£](https://json-schema.apifox.cn/)

```
get_weather_function = {
    'name': 'get_weather',
    'description': 'æŸ¥è¯¢å³æ—¶å¤©æ°”å‡½æ•°ï¼Œæ ¹æ®è¾“å…¥çš„åŸå¸‚åç§°ï¼ŒæŸ¥è¯¢å¯¹åº”åŸå¸‚çš„å®æ—¶å¤©æ°”',
    'parameters': {
        'type': 'object',
        'properties': { #å‚æ•°è¯´æ˜
            'loc': {
                'description': 'åŸå¸‚åç§°',
                'type': 'string'
            }
        },
        'required': ['loc']  #å¿…å¤‡å‚æ•°
    }
}
```

å®Œæˆå¯¹get_weatherå‡½æ•°æè¿°åï¼Œè¿˜éœ€è¦å°†å…¶åŠ å…¥toolsåˆ—è¡¨ï¼Œç”¨äºå‘ŠçŸ¥å¤§æ¨¡å‹å¯ä»¥ä½¿ç”¨å“ªäº›å‡½æ•°ä»¥åŠè¿™äº›å‡½æ•°å¯¹åº”çš„æè¿°ï¼Œå¹¶åœ¨å¯ç”¨å‡½æ•°å¯¹è±¡ä¸­è®°å½•ä¸€ä¸‹ï¼š
```
tools = [    
    {
        "type": "function",
        "function":get_weather_function
    }
]
available_functions = {
    'get_weather': get_weather
}
```

5. å¤§æ¨¡å‹è°ƒç”¨Function Call
æ¥ä¸‹æ¥ç”¨å¤§æ¨¡å‹è°ƒç”¨Function Call, è¿™é‡Œç”¨çš„å¤§æ¨¡å‹æ˜¯doubao-1-5-pro-32kã€‚

æ„é€ å¤§æ¨¡å‹ï¼š
```
llm = init_chat_model(
    model="doubao-1-5-pro-32k-250115",
    model_provider="openai",
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    api_key=os.environ.get("ARK_OPENAI_API_KEY"),
)
```

å…ˆè¯•ä¸‹ä¸ç”¨Function Call, è®©å¤§æ¨¡å‹æŸ¥è¯¢å¤©æ°”çš„ç»“æœ.

```
basic_qa_chain = model | StrOutputParser()
question = "è¯·å¸®æˆ‘æŸ¥è¯¢åŒ—äº¬åœ°åŒºä»Šæ—¥å¤©æ°”æƒ…å†µ"
result = basic_qa_chain.invoke(question)

print(result)
```

å¤§æ¨¡å‹ç»™å‡ºçš„è¾“å‡ºå¦‚ä¸‹ï¼š

```
æˆ‘æ²¡åŠæ³•ç›´æ¥è·å–å®æ—¶çš„åŒ—äº¬åœ°åŒºä»Šæ—¥å¤©æ°”æƒ…å†µã€‚ä¸è¿‡ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹å‡ ç§æ–¹å¼æŸ¥è¯¢ï¼š

å¤©æ°”ç±»åº”ç”¨ç¨‹åº
**å½©äº‘å¤©æ°”**ï¼šèƒ½æä¾›ç²¾å‡†çš„å¤©æ°”ä¿¡æ¯ï¼ŒåŒ…æ‹¬é€å°æ—¶é¢„æŠ¥ã€é™æ°´é¢„æŠ¥ç­‰ï¼Œè¿˜ä¼šæœ‰å¤©æ°”é›·è¾¾å›¾å±•ç¤ºé™æ°´åŠ¨æ€ã€‚
**å¢¨è¿¹å¤©æ°”**ï¼šé™¤äº†åŸºæœ¬çš„å¤©æ°”çŠ¶å†µã€æ¸©åº¦ã€æ¹¿åº¦ç­‰ä¿¡æ¯ï¼Œè¿˜æœ‰ç”Ÿæ´»æŒ‡æ•°ï¼Œå¦‚ç©¿è¡£æŒ‡æ•°ã€æ´—è½¦æŒ‡æ•°ç­‰ï¼Œæ–¹ä¾¿å®‰æ’æ—¥å¸¸ç”Ÿæ´»ã€‚
**ä¸­å›½å¤©æ°”é€š**ï¼šç”±ä¸­å›½æ°”è±¡å±€å®˜æ–¹æ¨å‡ºï¼Œæ•°æ®æƒå¨å¯é ï¼Œæœ‰è¯¦ç»†çš„æ°”è±¡é¢„è­¦ä¿¡æ¯ã€‚
```

å¯ä»¥çœ‹åˆ°ï¼Œæ²¡æœ‰Function Call åŠŸèƒ½ï¼Œå¤§æ¨¡å‹æ˜¯æŸ¥è¯¢åˆ°å®æ—¶å¤©æ°”çš„ã€‚

å°†å‡½æ•°ç›¸å…³ä¿¡æ¯è¾“å…¥ç»™å¤§æ¨¡å‹ï¼Œéœ€è¦é¢å¤–è®¾ç½®ä¸¤ä¸ªå‚æ•°ï¼Œé¦–å…ˆæ˜¯toolså‚æ•°, ç”¨äºç”³æ˜å¤–éƒ¨å‡½æ•°åº“, ä¹Ÿå°±æ˜¯æˆ‘ä»¬ä¸Šé¢å®šä¹‰çš„toolsåˆ—è¡¨å¯¹è±¡ã€‚å…¶æ¬¡æ˜¯å¯é€‰å‚æ•°tool_choiceå‚æ•°ï¼Œè¯¥å‚æ•°ç”¨äºæ§åˆ¶æ¨¡å‹å¯¹å‡½æ•°çš„é€‰å–ï¼Œé»˜è®¤å€¼ä¸ºauto, è¡¨ç¤ºä¼šæ ¹æ®ç”¨æˆ·æé—®è‡ªåŠ¨é€‰æ‹©è¦æ‰§è¡Œå‡½æ•°ï¼Œè‹¥æƒ³è®©æ¨¡å‹åœ¨æœ¬æ¬¡æ‰§è¡Œç‰¹å®šå‡½æ•°ä¸è¦è‡ªè¡ŒæŒ‘é€‰ï¼Œéœ€è¦ç»™tool_choiceå‚æ•°èµ‹äºˆ{"name":"functionname"}å€¼ï¼Œè¿™æ—¶å¤§æ¨¡å‹å°±ä¼šä»toolsåˆ—è¡¨ä¸­é€‰å–å‡½æ•°åä¸ºfunctionnameçš„å‡½æ•°æ‰§è¡Œã€‚è¿™é‡Œè®©æ¨¡å‹è‡ªåŠ¨æŒ‘é€‰å‡½æ•°æ¥æ‰§è¡Œ:
```
basic_qa_chain = model
question = "è¯·å¸®æˆ‘æŸ¥è¯¢åŒ—äº¬åœ°åŒºä»Šæ—¥å¤©æ°”æƒ…å†µ"
result = basic_qa_chain.invoke(question, tools=tools, tool_choice="auto")

print(result)
```

æ‰§è¡Œä¸Šé¢çš„ä»£ç , è¾“å‡ºå¦‚ä¸‹ï¼š

```
content='ç”¨æˆ·éœ€è¦æŸ¥è¯¢åŒ—äº¬åœ°åŒºä»Šæ—¥å¤©æ°”æƒ…å†µï¼Œè°ƒç”¨ get_weather å‡½æ•°è·å–ä¿¡æ¯ã€‚' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 79, 'total_tokens': 148, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'doubao-1-5-pro-32k-250115', 'system_fingerprint': None, 'id': '021766661440617e598b1e7ef00194e20b378e3561cb7f3f2d394', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None} id='lc_run--019b553a-43ee-76a1-9039-e51c3b831cc9-0' tool_calls=[{'name': 'get_weather', 'args': {'loc': 'åŒ—äº¬'}, 'id': 'call_jfncxiw8wf21j5h4jycfrolw', 'type': 'tool_call'}] usage_metadata={'input_tokens': 79, 'output_tokens': 69, 'total_tokens': 148, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}
```

å¯ä»¥çœ‹åˆ°ï¼Œå¤§æ¨¡å‹è¾“å‡ºäº†ä¸€ä¸ªå‡½æ•°è°ƒç”¨æŒ‡ä»¤ï¼Œè°ƒç”¨äº†get_weatherå‡½æ•°ï¼Œå‚æ•°ä¸ºloc=åŒ—äº¬ã€‚


æ„å¤§æ¨¡å‹ä¸ä¼šå¸®æˆ‘ä»¬è‡ªåŠ¨è°ƒç”¨å‡½æ•°ï¼Œå®ƒåªä¼šå¸®æˆ‘ä»¬é€‰æ‹©è¦è°ƒç”¨çš„å‡½æ•°ä»¥åŠç”Ÿæˆå‡½æ•°å‚æ•°ï¼Œ ä¸‹ä¸€æ­¥å°†å¤§æ¨¡å‹ç”Ÿæˆçš„å‡½æ•°å‚æ•°è¾“å…¥å¤§æ¨¡å‹é€‰æ‹©çš„å‡½æ•°å¹¶æ‰§è¡Œã€‚é€šè¿‡ä¸Šé¢å®šä¹‰çš„available_functionså¯¹è±¡æ‰¾åˆ°å…·ä½“çš„å‡½æ•°ï¼Œå¹¶å°†å¤§æ¨¡å‹è¿”å›çš„å‚æ•°ä¼ å…¥ï¼ˆè¿™é‡Œ ** æ˜¯ä¸€ç§ä¾¿æ·çš„å‚æ•°ä¼ é€’æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¼šå°†å­—å…¸ä¸­çš„æ¯ä¸ªkeyå¯¹åº”çš„valueä¼ è¾“åˆ°åŒåå‚æ•°ä½ä¸­ï¼‰,å¯ä»¥çœ‹åˆ°å¤©æ°”å‡½æ•°æˆåŠŸæ‰§è¡Œ:

```
# è·å–å‡½æ•°åç§°
function_name = result.tool_calls[0].function.name

# è·å¾—å¯¹åº”å‡½æ•°å¯¹è±¡
function_to_call = available_functions[function_name]

# è·å¾—æ‰§è¡Œå‡½æ•°æ‰€éœ€å‚æ•°
function_args = json.loads(result.tool_calls[0].function.arguments)

# æ‰§è¡Œå‡½æ•°
function_response = function_to_call(**function_args)

print(function_response)
```

è¾“å‡ºå¦‚ä¸‹å†…å®¹ï¼š
```
{'text': 'æ™´', 'code': '1', 'temperature': '-1'}
```
ä¸Šé¢çš„è¾“å‡ºç»“æœä¹Ÿå°±æ˜¯çŸ¥å¿ƒå¤©æ°”æŸ¥è¯¢ç»“æœï¼Œtextä¸ºæ™´ï¼Œcodeä¸º1ï¼Œtemperatureä¸º-1æ‘„æ°åº¦ã€‚

åœ¨è°ƒç”¨å¤©æ°”å‡½æ•°å¾—åˆ°å¤©æ°”æƒ…å†µåï¼Œå°†å¤©æ°”ç»“æœä¼ å…¥mesagesåˆ—è¡¨ä¸­å¹¶å‘é€ç»™å¤§æ¨¡å‹ï¼Œè®©å¤§æ¨¡å‹ç†è§£ä¸Šä¸‹æ–‡ã€‚å‡½æ•°æ‰§è¡Œç»“æœçš„messageæ˜¯tool_messageç±»å‹ã€‚

å°†å¤§æ¨¡å‹å…³äºé€‰æ‹©å‡½æ•°çš„å›å¤response_messageå†…å®¹è§£æåä¼ å…¥messagesåˆ—è¡¨ä¸­ã€‚

```
print(response_message.model_dump())
messages.append(response_message.model_dump()) 
```

ä¸Šé¢çš„model_dump()è¾“å‡ºçš„å†…å®¹å¦‚ä¸‹ï¼š
```
{
	'content': '',
	'refusal': None,
	'role': 'assistant',
	'annotations': None,
	'audio': None,
	'function_call': None,
	'tool_calls': [{
		'id': 'call_0_8feaa367-c274-4c84-830f-13b49358a231',
		'function': {
			'arguments': '{"loc":"åŒ—äº¬"}',
			'name': 'get_weather'
		},
		'type': 'function',
		'index': 0
	}]
}
```

å†å°†å‡½æ•°æ‰§è¡Œç»“æœä½œä¸ºtool_messageå¹¶ä¸response_messageå…³è”åä¼ å…¥messagesåˆ—è¡¨ä¸­:

```
messages.append({
    "role": "tool",
    "content": json.dumps(function_response), # å°†å›å¤çš„å­—å…¸è½¬åŒ–ä¸ºjsonå­—ç¬¦ä¸²
    "tool_call_id": response_message.tool_calls[0].id # å°†å‡½æ•°æ‰§è¡Œç»“æœä½œä¸ºtool_messageæ·»åŠ åˆ°messagesä¸­, å¹¶å…³è”è¿”å›æ‰§è¡Œå‡½æ•°å†…å®¹çš„id
})
```

æ¥ä¸‹æ¥ï¼Œå†æ¬¡è°ƒç”¨å¤§æ¨¡å‹æ¥å›´ç»•messagesè¿›è¡Œå›ç­”ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ­¤æ—¶ä¸å†éœ€è¦å‘æ¨¡å‹é‡å¤æé—®ï¼Œåªéœ€è¦ç®€å•çš„å°†æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½çš„messages ä¼ ç»™å¤§æ¨¡å‹ã€‚

```
second_response = model.invoke(messages)
print(second_response.content)
```

ä¸‹é¢çœ‹å¤§æ¨¡å‹çš„è¾“å‡ºç»“æœï¼Œå¾ˆæ˜æ˜¾å¤§æ¨¡å‹æ¥æ”¶åˆ°äº†å‡½æ•°æ‰§è¡Œçš„ç»“æœï¼Œå¹¶è¿›ä¸€æ­¥å¤„ç†å¾—åˆ°è¾“å‡ºï¼ŒåŒæ—¶å¤©æ°”å’Œæ°”æ¸©çš„è¾“å‡ºä¹Ÿæ˜¯æ­£ç¡®çš„ã€‚

ä¹Ÿå¯ä»¥å†™ä¸€ä¸ªä»£ç ï¼Œå°†ä¸Šé¢çš„æµç¨‹ä¸²èµ·æ¥ï¼Œè¿™æ ·ä½¿ç”¨èµ·æ¥å°±ç®€å•äº†ã€‚ä¸‹é¢æ˜¯å®Œæ•´çš„ä»£ç ï¼š

```
import os
import requests
import json
from langchain.chat_models import init_chat_model
from langchain_core.output_parsers import StrOutputParser

# åˆå§‹åŒ–æ¨¡å‹ï¼ˆä½¿ç”¨è±†åŒ…æ¨¡å‹ï¼‰
model = init_chat_model(
    model="doubao-1-5-pro-32k-250115",
    model_provider="openai",
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    api_key=os.environ["ARK_OPENAI_API_KEY"],
)

# æŸ¥è¯¢å¤©æ°”å‡½æ•°
def get_weather(loc):
    """æŸ¥è¯¢å³æ—¶å¤©æ°”å‡½æ•°ï¼Œæ ¹æ®è¾“å…¥çš„åŸå¸‚åç§°ï¼ŒæŸ¥è¯¢å¯¹åº”åŸå¸‚çš„å®æ—¶å¤©æ°”"""
    url = "https://api.seniverse.com/v3/weather/now.json"
    params = {
        "key": os.environ["XINZHI_API_KEY"],  # å¡«å†™ä½ çš„ç§é’¥
        "location": loc,
        "language": "zh-Hans",
        "unit": "c",
    }
    response = requests.get(url, params=params)
    temperature = response.json()
    return temperature['results'][0]['now']

# å¦ä¸€ä¸ªç¤ºä¾‹å‡½æ•°ï¼šè·å–åŸå¸‚ä¿¡æ¯
def get_city_info(city):
    """è·å–åŸå¸‚åŸºæœ¬ä¿¡æ¯"""
    # è¿™é‡Œå¯ä»¥æ¥å…¥å…¶ä»–APIï¼Œæ¯”å¦‚è·å–åŸå¸‚äººå£ã€é¢ç§¯ç­‰ä¿¡æ¯
    city_data = {
        "åŒ—äº¬": {"population": "2154ä¸‡", "area": "16410å¹³æ–¹å…¬é‡Œ", "description": "ä¸­åäººæ°‘å…±å’Œå›½çš„é¦–éƒ½"},
        "ä¸Šæµ·": {"population": "2489ä¸‡", "area": "6341å¹³æ–¹å…¬é‡Œ", "description": "ä¸­å›½çš„ç»æµä¸­å¿ƒåŸå¸‚"},
        "å¹¿å·": {"population": "1530ä¸‡", "area": "7434å¹³æ–¹å…¬é‡Œ", "description": "å¹¿ä¸œçœçœä¼šï¼Œé‡è¦çš„æ¸¯å£åŸå¸‚"},
    }
    return city_data.get(city, {"error": "æœªæ‰¾åˆ°è¯¥åŸå¸‚ä¿¡æ¯"})

def run_conv(messages, 
             api_key, 
             tools=None, 
             functions_list=None, 
             model_name="doubao-1-5-pro-32k-250115",
             max_tool_calls=5):
    """
    é€šç”¨çš„å¯¹è¯å‡½æ•°ï¼Œæ”¯æŒå·¥å…·è°ƒç”¨ï¼ˆå¢å¼ºç‰ˆï¼Œæ”¯æŒå¤šæ¬¡è°ƒç”¨ï¼‰
    
    Args:
        messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
        api_key: APIå¯†é’¥
        tools: å·¥å…·åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        functions_list: å¯ç”¨å‡½æ•°åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        model_name: æ¨¡å‹åç§°
        max_tool_calls: æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°ï¼Œé˜²æ­¢æ— é™å¾ªç¯
    
    Returns:
        str: æœ€ç»ˆå“åº”å†…å®¹
    """
    # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ç¡®ä¿API keyæ­£ç¡®
    import os
    original_api_key = os.environ.get("ARK_OPENAI_API_KEY")
    os.environ["ARK_OPENAI_API_KEY"] = api_key
    
    try:
        # ä½¿ç”¨ç°æœ‰çš„å…¨å±€æ¨¡å‹å®ä¾‹
        user_messages = messages.copy()
        
        # å¦‚æœæ²¡æœ‰å¤–éƒ¨å‡½æ•°åº“ï¼Œåˆ™æ‰§è¡Œæ™®é€šçš„å¯¹è¯ä»»åŠ¡
        if tools is None:
            response = model.invoke(user_messages)
            final_response = response.content
        
        # è‹¥å­˜åœ¨å¤–éƒ¨å‡½æ•°åº“ï¼Œæ”¯æŒå¤šæ¬¡å·¥å…·è°ƒç”¨
        else:
            # åˆ›å»ºå¤–éƒ¨å‡½æ•°åº“å­—å…¸
            available_functions = {func.__name__: func for func in functions_list}
            
            # å¾ªç¯å¤„ç†å·¥å…·è°ƒç”¨
            tool_call_count = 0
            
            while tool_call_count < max_tool_calls:
                # è°ƒç”¨æ¨¡å‹
                response = model.invoke(
                    user_messages, 
                    tools=tools, 
                    tool_choice="auto"
                )
                response_message = response
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                if hasattr(response_message, 'tool_calls') and response_message.tool_calls:
                    # å¤„ç†æ‰€æœ‰å·¥å…·è°ƒç”¨
                    for tool_call in response_message.tool_calls:
                        # è·å–å‡½æ•°åå’Œå‚æ•°
                        function_name = tool_call["name"]
                        function_args = tool_call["args"]
                        tool_call_id = tool_call["id"]
                        
                        # æ£€æŸ¥å‡½æ•°æ˜¯å¦å­˜åœ¨
                        if function_name in available_functions:
                            function_to_call = available_functions[function_name]
                            
                            # æ‰§è¡Œå‡½æ•°
                            try:
                                function_response = function_to_call(**function_args)
                                
                                # å°†æ¨¡å‹å“åº”å’Œå·¥å…·å“åº”æ·»åŠ åˆ°æ¶ˆæ¯å†å²
                                user_messages.append(response_message.model_dump())
                                user_messages.append({
                                    "role": "tool",
                                    "content": json.dumps(function_response, ensure_ascii=False),
                                    "tool_call_id": tool_call_id
                                })
                                
                            except Exception as e:
                                print(f"å‡½æ•°æ‰§è¡Œå‡ºé”™: {e}")
                                # æ·»åŠ é”™è¯¯ä¿¡æ¯åˆ°æ¶ˆæ¯å†å²
                                user_messages.append(response_message.model_dump())
                                user_messages.append({
                                    "role": "tool",
                                    "content": json.dumps({"error": str(e)}),
                                    "tool_call_id": tool_call_id
                                })
                        else:
                            print(f"æœªæ‰¾åˆ°å‡½æ•°: {function_name}")
                    
                    tool_call_count += 1
                    
                else:
                    # æ¨¡å‹æœªé€‰æ‹©å·¥å…·è°ƒç”¨ï¼Œç»“æŸå¾ªç¯
                    break
            
            # è·å–æœ€ç»ˆå“åº”
            if user_messages:
                final_response = response_message.content
            else:
                final_response = "æœªèƒ½è·å¾—æœ‰æ•ˆå“åº”"
        
        return final_response
    
    finally:
        # æ¢å¤åŸæ¥çš„API key
        if original_api_key is not None:
            os.environ["ARK_OPENAI_API_KEY"] = original_api_key

# å®šä¹‰å·¥å…·
get_weather_function = {
    'name': 'get_weather',
    'description': 'æŸ¥è¯¢å³æ—¶å¤©æ°”å‡½æ•°ï¼Œæ ¹æ®è¾“å…¥çš„åŸå¸‚åç§°ï¼ŒæŸ¥è¯¢å¯¹åº”åŸå¸‚çš„å®æ—¶å¤©æ°”',
    'parameters': {
        'type': 'object',
        'properties': {  # å‚æ•°è¯´æ˜
            'loc': {
                'description': 'åŸå¸‚åç§°',
                'type': 'string'
            }
        },
        'required': ['loc']  # å¿…å¤‡å‚æ•°
    }
}

get_city_info_function = {
    'name': 'get_city_info',
    'description': 'è·å–åŸå¸‚åŸºæœ¬ä¿¡æ¯ï¼ŒåŒ…æ‹¬äººå£ã€é¢ç§¯å’Œæè¿°',
    'parameters': {
        'type': 'object',
        'properties': {
            'city': {
                'description': 'åŸå¸‚åç§°',
                'type': 'string'
            }
        },
        'required': ['city']
    }
}

tools = [
    {
        "type": "function",
        "function": get_weather_function
    },
    {
        "type": "function", 
        "function": get_city_info_function
    }
]

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # è®¾ç½®APIå¯†é’¥
    ds_api_key = os.environ["ARK_OPENAI_API_KEY"]
    
    # æµ‹è¯•1ï¼šæ™®é€šå¯¹è¯ï¼ˆæ— å·¥å…·ï¼‰
    print("=== æµ‹è¯•1ï¼šæ™®é€šå¯¹è¯ ===")
    messages1 = [{"role": "user", "content": "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²"}]
    response1 = run_conv(messages=messages1, api_key=ds_api_key)
    print(f"AIå›ç­”ï¼š{response1}")
    print()
    
    # æµ‹è¯•2ï¼šå·¥å…·è°ƒç”¨ï¼ˆå¤©æ°”æŸ¥è¯¢ï¼‰
    print("=== æµ‹è¯•2ï¼šå¤©æ°”æŸ¥è¯¢ ===")
    messages2 = [{"role": "user", "content": "è¯·é—®ä¸Šæµ·ä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿ"}]
    response2 = run_conv(messages=messages2, 
                        api_key=ds_api_key,
                        tools=tools, 
                        functions_list=[get_weather, get_city_info])
    print(f"AIå›ç­”ï¼š{response2}")
    print()
    
    # æµ‹è¯•3ï¼šå·¥å…·è°ƒç”¨ï¼ˆåŸå¸‚ä¿¡æ¯ï¼‰
    print("=== æµ‹è¯•3ï¼šåŸå¸‚ä¿¡æ¯æŸ¥è¯¢ ===")
    messages3 = [{"role": "user", "content": "è¯·å‘Šè¯‰æˆ‘åŒ—äº¬çš„åŸºæœ¬ä¿¡æ¯"}]
    response3 = run_conv(messages=messages3,
                        api_key=ds_api_key, 
                        tools=tools,
                        functions_list=[get_weather, get_city_info])
    print(f"AIå›ç­”ï¼š{response3}")
    print()
    
    # æµ‹è¯•4ï¼šå¤åˆé—®é¢˜ï¼ˆéœ€è¦å¤šä¸ªå·¥å…·ï¼‰
    print("=== æµ‹è¯•4ï¼šå¤åˆé—®é¢˜ï¼ˆå¤šæ¬¡å·¥å…·è°ƒç”¨ï¼‰ ===")
    messages4 = [{"role": "user", "content": "æˆ‘æƒ³äº†è§£å¹¿å·çš„å¤©æ°”å’ŒåŸºæœ¬ä¿¡æ¯"}]
    print(f"é—®é¢˜: {messages4[0]['content']}")
    print("å¼€å§‹å¤„ç†...")
    
    response4 = run_conv(messages=messages4,
                        api_key=ds_api_key,
                        tools=tools, 
                        functions_list=[get_weather, get_city_info],
                        max_tool_calls=5)
    
    print(f"AIå›ç­”ï¼š{response4}")
    print("=" * 50)
```


å…¶å®å¯ä»¥ä¸ç”¨ä¸Šé¢çš„æ–¹å¼è¿™ä¹ˆå¤æ‚ï¼Œå¯ä»¥ç›´æ¥ç”¨create_agentå‡½æ•°å®ç°ï¼Œè¯¦ç»†ä»£ç è§chain9.pyã€‚

```
import requests
import os

from langchain.agents import create_agent
from langchain.tools import tool
from langchain_core.prompts import ChatPromptTemplate
from langchain.chat_models import init_chat_model


@tool
def get_weather(loc: str):
    """æŸ¥è¯¢å³æ—¶å¤©æ°”å‡½æ•°ï¼Œæ ¹æ®è¾“å…¥çš„åŸå¸‚åç§°ï¼ŒæŸ¥è¯¢å¯¹åº”åŸå¸‚çš„å®æ—¶å¤©æ°”"""
    url = "https://api.seniverse.com/v3/weather/now.json"
    params = {
        "key": os.environ["XINZHI_API_KEY"],
        "location": loc,
        "language": "zh-Hans",
        "unit": "c",
    }
    response = requests.get(url, params=params)
    data = response.json()
    return data["results"][0]["now"]


prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "ä½ æ˜¯å¤©æ°”åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºç›¸åº”çš„å¤©æ°”ä¿¡æ¯"),
        ("human", "{input}"),
        ("placeholder", "{agent_scratchpad}"),
    ]
)

# Doubao / ç«å±±æ¨¡å‹
model = init_chat_model(
    model="doubao-1-5-pro-32k-250115",
    model_provider="openai",
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    api_key=os.environ["ARK_OPENAI_API_KEY"],
)

# åˆ›å»º agent
agent = create_agent(
    model=model,
    tools=[get_weather],
    system_prompt="ä½ æ˜¯å¤©æ°”åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºç›¸åº”çš„å¤©æ°”ä¿¡æ¯",
)


# è°ƒç”¨
result = agent.invoke(
    {"messages": [("user", "åŒ—äº¬çš„å¤©æ°”")]}
)


final_message = result["messages"][-1]
print(final_message.content)

```