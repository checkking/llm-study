# LangChain å­¦ä¹ 
## LangChain æ¦‚è¿°
LangChain æä¾›äº†é¢„æ„å»ºçš„æ™ºèƒ½ä½“æ¶æ„å’Œæ¨¡å‹é›†æˆï¼Œå¸®åŠ©æˆ‘ä»¬å¿«é€Ÿä¸Šæ‰‹ï¼Œæ— ç¼åœ°å°†å¤§è¯­è¨€æ¨¡å‹èå…¥æ™ºèƒ½ä½“å’Œåº”ç”¨ä¸­ã€‚
å¦‚æœæ‚¨å¸Œæœ›å¿«é€Ÿæ„å»ºæ™ºèƒ½ä½“å’Œè‡ªä¸»åº”ç”¨ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨ä½¿ç”¨LangChainã€‚å½“æ‚¨æœ‰æ›´é«˜çº§çš„éœ€æ±‚ï¼Œéœ€è¦ç»“åˆç¡®å®šæ€§å·¥ä½œæµä¸æ™ºèƒ½ä½“å·¥ä½œæµã€è¿›è¡Œæ·±åº¦å®šåˆ¶å¹¶ä¸¥æ ¼æ§åˆ¶å»¶è¿Ÿæ—¶ï¼Œè¯·ä½¿ç”¨LangGraphâ€”â€”æˆ‘ä»¬çš„ä½çº§æ™ºèƒ½ä½“ç¼–æ’æ¡†æ¶å’Œè¿è¡Œæ—¶ã€‚

LangChainä»£ç†æ„å»ºäºLangGraphä¹‹ä¸Šï¼Œä»¥æä¾›æŒä¹…æ‰§è¡Œã€æµå¼å¤„ç†ã€äººæœºååŒã€æŒä¹…åŒ–ç­‰åŠŸèƒ½ã€‚æ‚¨æ— éœ€äº†è§£LangGraphï¼Œå³å¯è½»æ¾ä½¿ç”¨åŸºæœ¬çš„LangChainä»£ç†ã€‚

## Langchain å®‰è£…
```bash
pip install -q langchain
```

## LangChain åŸºç¡€æ™ºèƒ½ä½“å®ç°

```
from langchain.agents import create_agent

def get_weather(city: str) -> str:
    """Get weather for a given city."""
    return f"It's always sunny in {city}!"

agent = create_agent(
    model="claude-sonnet-4-5-20250929",
    tools=[get_weather],
    system_prompt="You are a helpful assistant",
)

# Run the agent
agent.invoke(
    {"messages": [{"role": "user", "content": "what is the weather in sf"}]}
)
```

ä¸Šé¢çš„æ–¹å¼éœ€è¦æœ¬åœ°éƒ¨ç½²å¥½å¤§æ¨¡å‹ï¼Œæˆ–è€…ç”¨ Claude (Anthropic)çš„API KEYè´¦å·ï¼Œ å¹¶è®¾ç½®ANTHROPIC_API_KEYç¯å¢ƒå˜é‡ã€‚å› æ¡ä»¶åŸå› ï¼Œä¸Šé¢ä»£ç ä¸èƒ½è¿è¡Œã€‚

## LangChain æ™ºèƒ½ä½“å®ç°ï¼ˆOpenAIï¼‰

```
# 1. å¯¼å…¥å¿…è¦çš„æ¨¡å—
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent


# 2. å°†å‡½æ•°å®šä¹‰ä¸º LangChain çš„ Tool
def get_weather(city: str) -> str:
    """Get the current weather in a given city."""
    # è¿™é‡Œæ˜¯æ¨¡æ‹Ÿï¼ŒçœŸå®åº”ç”¨ä¸­åº”æ¥å…¥å¤©æ°”API
    return f"The weather in {city} is sunny and 72Â°F."

# 3. ä»¥ OpenAI å…¼å®¹æ¨¡å¼åˆå§‹åŒ–èŠå¤©æ¨¡å‹
# å…³é”®ï¼šé€šè¿‡æŒ‡å®š base_url æ¥å…¼å®¹ä¸åŒæœåŠ¡å•†
llm = ChatOpenAI(
    model="deepseek-chat",  # æ›¿æ¢ä¸ºè±†åŒ…çš„æ¨¡å‹åï¼Œå¦‚ "Doubao-lite-32k"
    openai_api_key="{your key}",  # æ›¿æ¢ä¸ºä½ çš„APIå¯†é’¥
    base_url="https://api.deepseek.com/v1",  # æ›¿æ¢ä¸ºè±†åŒ…çš„APIç«¯ç‚¹
    temperature=0
)

# 4. å®šä¹‰æ™ºèƒ½ä½“çš„ç³»ç»Ÿæç¤ºè¯
system_prompt = "You are a helpful assistant that can provide weather information."

# 5. åˆ›å»ºæ™ºèƒ½ä½“
agent = create_agent(
    model=llm,
    tools=[get_weather],
    system_prompt=system_prompt
)

# 6. æ‰§è¡ŒæŸ¥è¯¢
result = agent.invoke(
    {"messages": [{"role": "user", "content": "What is the weather in San Francisco?"}]}
)

print(result)
```

æ³¨æ„ï¼š å¯†é’¥ä¸è¦æ”¾åˆ°ä»£ç ä¸­ï¼Œåº”è¯¥ä»ç¯å¢ƒå˜é‡ä¸­è·å–ã€‚ å¦‚æœç”¨condaç®¡ç†ç¯å¢ƒï¼Œå¯ä»¥åœ¨å½“å‰ç¯å¢ƒä¸‹è®¾ç½®ç¯å¢ƒå˜é‡.

```
conda env config vars set OPENAI_API_KEY={your key}
```

è®¾ç½®å®Œæˆåï¼Œå¿…é¡»å…ˆåœç”¨å†é‡æ–°æ¿€æ´»ç¯å¢ƒï¼Œå˜é‡æ‰ä¼šåŠ è½½ï¼š
```
conda deactivate
conda activate {your env}
```

### ç«å±±å¼•æ“Doubaoæ¨¡å‹æ¥å…¥å…³é”®ä»£ç 

```
llm_doubao = ChatOpenAI(
    model="doubao-1-5-pro-32k-250115",  # æ›¿æ¢ä¸ºè±†åŒ…çš„æ¨¡å‹åï¼Œå¦‚ "Doubao-lite-32k"
    openai_api_key=os.environ["ARK_OPENAI_API_KEY"],  # æ›¿æ¢ä¸ºä½ çš„APIå¯†é’¥
    base_url="https://ark.cn-beijing.volces.com/api/v3",  # æ›¿æ¢ä¸ºç«å±±å¼•æ“çš„APIç«¯ç‚¹
    temperature=0
)

system_prompt = "You are a helpful assistant that can provide weather information."

agent_doubao = create_agent(
    model=llm_doubao,
    tools=[get_weather],
    system_prompt=system_prompt
)

result_doubao = agent_doubao.invoke(
    {"messages": [{"role": "user", "content": "What is the weather in San Francisco?"}]}
)
print(result_doubao)
```

## åˆ›å»ºä¸€ä¸ªçœŸå®ä¸–ç•Œçš„æ™ºèƒ½ä½“
æ„å»ºä¸€ä¸ªå®ç”¨çš„å¤©æ°”é¢„æŠ¥ä»£ç†ï¼Œä»¥å±•ç¤ºå…³é”®çš„ç”Ÿäº§æ¦‚å¿µï¼š

1. ç»†åŒ–ç³»ç»Ÿæç¤ºä»¥æ”¹å–„æ™ºèƒ½ä½“è¡Œä¸º
2. åˆ›å»ºå¯ä¸å¤–éƒ¨æ•°æ®é›†æˆçš„å·¥å…·
3. æ¨¡å‹é…ç½®ä»¥å®ç°ä¸€è‡´çš„å›å¤
4. ç»“æ„åŒ–è¾“å‡ºä»¥è·å¾—å¯é¢„æµ‹çš„ç»“æœ
5. å¯¹è¯è®°å¿†ç”¨äºèŠå¤©å¼äº¤äº’
6. åˆ›å»ºå¹¶è¿è¡Œæ™ºèƒ½ä½“åˆ›å»ºä¸€ä¸ªåŠŸèƒ½å®Œå¤‡çš„æ™ºèƒ½ä½“

### å®šä¹‰ç³»ç»Ÿæç¤ºè¯
ç³»ç»Ÿæç¤ºè¯å®šä¹‰æ™ºèƒ½ä½“çš„è§’è‰²å’Œè¡Œä¸ºï¼Œä¿æŒå…·ä½“ä¸”å¯æ“ä½œï¼š
```
SYSTEM_PROMPT = """You are an expert weather forecaster, who speaks in puns.

You have access to two tools:

- get_weather_for_location: use this to get the weather for a specific location
- get_user_location: use this to get the user's location

If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location."""
```

### åˆ›å»ºå·¥å…·
å·¥å…·å¯ä»¥è®©æ¨¡å‹é€šè¿‡æˆ‘ä»¬å®šä¹‰çš„å‡½æ•°è°ƒç”¨çš„æ–¹å¼ä¸å¤–éƒ¨ç³»ç»Ÿäº¤äº’ï¼Œå·¥å…·ä¹Ÿå¯ä»¥ä¾èµ–è¿è¡Œæ—¶å’Œæ™ºèƒ½ä½“è®°å¿†ã€‚

```
from dataclasses import dataclass
from langchain.tools import tool, ToolRuntime

@tool
def get_weather_for_location(city: str) -> str:
    """Get weather for a given city."""
    return f"It's always sunny in {city}!"

@dataclass
class Context:
    """Custom runtime context schema."""
    user_id: str

@tool
def get_user_location(runtime: ToolRuntime[Context]) -> str:
    """Retrieve user information based on user ID."""
    user_id = runtime.context.user_id
    return "Florida" if user_id == "1" else "SF"
```

* é…ç½®æ¨¡å‹

æ ¹æ®ä¸‹é¢çš„ä»£ç ï¼Œé…ç½®æ¨¡å‹å‚æ•°ï¼Œæ”¹æˆè‡ªå·±çš„å‚æ•°ï¼š

```
from langchain.chat_models import init_chat_model

model = init_chat_model(
    "claude-sonnet-4-5-20250929",
    temperature=0.5,
    timeout=10,
    max_tokens=1000
)
```

* å®šä¹‰å›å¤æ ¼å¼
æˆ‘ä»¬å¯ä»¥é€šè¿‡å®šä¹‰ç»“æ„åŒ–çš„å›å¤æ ¼å¼ï¼Œæ¥ç¡®ä¿æ™ºèƒ½ä½“çš„å›å¤ç¬¦åˆæˆ‘ä»¬çš„é¢„æœŸã€‚

```
from dataclasses import dataclass

# We use a dataclass here, but Pydantic models are also supported.
@dataclass
class ResponseFormat:
    """Response schema for the agent."""
    # A punny response (always required)
    punny_response: str
    # Any interesting information about the weather if available
    weather_conditions: str | None = None
```

* æ·»åŠ è®°å¿†
ä¸ºäº†ç»´æŠ¤ä¸æ™ºèƒ½ä½“äº¤äº’å·¥ç¨‹ä¸­çš„çŠ¶æ€ï¼Œæˆ‘ä»¬å¯ä»¥æ·»åŠ å¯¹è¯è®°å¿†ï¼Œè¿™æ ·æ™ºèƒ½ä½“å°±èƒ½è®°ä½ä¹‹å‰çš„å¯¹è¯å’Œä¸Šä¸‹æ–‡ã€‚

```
from langgraph.checkpoint.memory import InMemorySaver

checkpointer = InMemorySaver()
```

* åˆ›å»ºå¹¶è¿è¡Œæ™ºèƒ½ä½“

å°†æ‰€æœ‰ç»„ä»¶ç»„è£…åˆ°æ™ºèƒ½ä½“ä¸­ï¼Œå¹¶è¿è¡Œã€‚

```
from langchain.agents.structured_output import ToolStrategy

agent = create_agent(
    model=model,
    system_prompt=SYSTEM_PROMPT,
    tools=[get_user_location, get_weather_for_location],
    context_schema=Context,
    response_format=ToolStrategy(ResponseFormat),
    checkpointer=checkpointer
)

# `thread_id` is a unique identifier for a given conversation.
config = {"configurable": {"thread_id": "1"}}

response = agent.invoke(
    {"messages": [{"role": "user", "content": "what is the weather outside?"}]},
    config=config,
    context=Context(user_id="1")
)

print(response['structured_response'])
# ResponseFormat(
#     punny_response="Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!",
#     weather_conditions="It's always sunny in Florida!"
# )


# Note that we can continue the conversation using the same `thread_id`.
response = agent.invoke(
    {"messages": [{"role": "user", "content": "thank you!"}]},
    config=config,
    context=Context(user_id="1")
)

print(response['structured_response'])
# ResponseFormat(
#     punny_response="You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!",
#     weather_conditions=None
# )
```

## LangChain ä¸­çš„â€œé“¾â€
é“¾å¼è°ƒç”¨ä½äºLangChainä¸‰å±‚æ ¸å¿ƒæ¶æ„ä¸­çš„ä¸­é—´å±‚â€”â€”å·¥ä½œæµAPIæŠ½è±¡å±‚ã€‚Chainç¿»è¯‘æˆä¸­æ–‡å°±æ˜¯â€œé“¾â€ï¼Œæˆ‘ä»¬å°†å¤§æ¨¡å‹ã€ç›¸å…³å·¥å…·ç­‰ä½œä¸ºç»„ä»¶ï¼Œé“¾å°±æ˜¯è´Ÿè´£å°†è¿™äº›ç»„ä»¶æŒ‰ç…§æŸä¸€ç§é€»è¾‘ï¼Œé¡ºåºç»„åˆæˆä¸€ä¸ªæµæ°´çº¿çš„æ–¹å¼ã€‚æ¯”å¦‚æˆ‘ä»¬è¦æ„å»ºä¸€ä¸ªç®€å•çš„é—®ç­”é“¾ï¼Œå°±éœ€è¦æŠŠå¤§æ¨¡å‹ç»„ä»¶å’Œæ ‡å‡†è¾“å‡ºç»„ä»¶ç”¨é“¾ä¸²è”èµ·æ¥ã€‚

### LangChain é“¾ä»£ç å®ç°
1. ç®€å•é“¾

æ­å»ºä¸€ä¸ªç®€å•é“¾ï¼Œå°†æ¨¡å‹â€œè¾“å‡ºç»“æœâ€è¿‡æ»¤ä¸ºä¸€ä¸ªçº¯å­—ç¬¦ä¸²æ ¼å¼:

```
from langchain.chat_models import init_chat_model
from langchain_core.output_parsers import StrOutputParser # å¯¼å…¥æ ‡å‡†è¾“å‡ºç»„ä»¶

model = init_chat_model(
    model="doubao-1-5-pro-32k-250115",
    model_provider="openai",
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    api_key=os.environ["ARK_OPENAI_API_KEY"], #ä½ æ³¨å†Œçš„ç«å±±å¼•æ“api_key
)

# æ­å»ºé“¾æ¡ï¼ŒæŠŠmodelå’Œå­—ç¬¦ä¸²è¾“å‡ºè§£æå™¨ç»„ä»¶è¿æ¥åœ¨ä¸€èµ·
basic_qa_chain = model | StrOutputParser()

# æŸ¥çœ‹è¾“å‡ºç»“æœ
question = "ä½ å¥½ï¼Œè¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
result = basic_qa_chain.invoke(question)

print(result)

```

è¿è¡Œä¸Šé¢çš„ä»£ç ï¼Œå¯ä»¥çœ‹åˆ°æ­¤æ—¶çš„resultä¸å†æ˜¯åŒ…å«æ¨¡å‹å„ç§è°ƒç”¨ä¿¡æ¯çš„AIMessageå¯¹è±¡ï¼Œè€Œæ˜¯çº¯ç²¹çš„æ¨¡å‹å“åº”çš„å­—ç¬¦ä¸²ç»“æœã€‚


2. æç¤ºè¯æ¨¡æ¿åˆ›å»ºé“¾
é“¾æµç¨‹å¢åŠ ä¸€ä¸ªæç¤ºè¯æ¨¡æ¿ï¼Œå¯ä»¥å€ŸåŠ©ChatPromptTemplateéå¸¸ä¾¿æ·çš„å°†ä¸€ä¸ªæç¤ºè¯æ¨¡æ¿æ‰“é€ ä¸ºç»„ä»¶ï¼ŒåŒæ ·ä»¥é“¾çš„å½¢å¼åŠ å…¥å½“å‰æµç¨‹ä¸­

```
import os
from langchain.chat_models import init_chat_model
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

model = init_chat_model(
    model="doubao-1-5-pro-32k-250115",
    model_provider="openai",
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    api_key=os.environ["ARK_OPENAI_API_KEY"], #ä½ æ³¨å†Œçš„ç«å±±å¼•æ“api_key
)

prompt_template = ChatPromptTemplate([
    ("system", "ä½ æ˜¯ä¸€ä¸ªä¹æ„åŠ©äººçš„åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç»™å‡ºå›ç­”"),
    ("user", "è¿™æ˜¯ç”¨æˆ·çš„é—®é¢˜ï¼š {topic}ï¼Œ è¯·ç”¨ yes æˆ– no æ¥å›ç­”")
])

# ç›´æ¥ä½¿ç”¨æ¨¡å‹ + è¾“å‡ºè§£æå™¨
bool_qa_chain = prompt_template | model | StrOutputParser()
# æµ‹è¯•
question = "è¯·é—® 1 + 1 æ˜¯å¦ å¤§äº 2ï¼Ÿ"
result = bool_qa_chain.invoke({'topic':question})
print(result)

```

å€ŸåŠ©æç¤ºè¯æ¨¡æ¿å³å¯å®ç°ç›¸åº”çš„ç»“æ„åŒ–è¾“å‡ºã€‚


3. ç»“æ„åŒ–è§£æå™¨

LangChainä¸­ä¸€ä¸ªåŸºç¡€çš„é“¾ä¸€èˆ¬ç”±å¦‚ä¸‹ä¸‰éƒ¨åˆ†æ„æˆï¼Œåˆ†åˆ«æ˜¯æç¤ºè¯æ¨¡æ¿ã€å¤§æ¨¡å‹å’Œç»“æ„åŒ–è§£æå™¨ã€‚æ™ºèƒ½ä½“å¼€å‘äººå‘˜é€šè¿‡æç¤ºè¯è®©å¤§æ¨¡å‹è¾“å‡ºç»“æ„åŒ–çš„å­—ç¬¦ä¸²ï¼Œç„¶åé€šè¿‡ç»“æ„åŒ–è§£æå™¨å°†å­—ç¬¦ä¸²è§£æä¸ºæŒ‡å®šå¯¹è±¡ã€‚æµç¨‹ä¸º:


```mermaid
graph TD
    A[ç”¨æˆ·è¾“å…¥] --> B[PromptTemplate <br/>æç¤ºè¯æ¨¡æ¿]
    B --> C[ChatModel <br/>å¤§æ¨¡å‹]
    C --> D[OutputParser <br/>ç»“æ„åŒ–è§£æ]
    D --> E[ç»“æ„åŒ–ç»“æœ]
```


LangChainä¸­å¸¸ç”¨çš„æ ¸å¿ƒç»“æ„åŒ–è§£æå™¨åŠŸèƒ½å¦‚ä¸‹:


| è§£æå™¨åç§° | åŠŸèƒ½æè¿° | ç±»å‹ |
|-----------|----------|------|
| BooleanOutputParser | å°† LLM è¾“å‡ºè§£æä¸ºå¸ƒå°”å€¼ | åŸºç¡€ç±»å‹è§£æ |
| DatetimeOutputParser | å°† LLM è¾“å‡ºè§£æä¸ºæ—¥æœŸæ—¶é—´ | åŸºç¡€ç±»å‹è§£æ |
| EnumOutputParser | è§£æè¾“å‡ºä¸ºé¢„å®šä¹‰æšä¸¾å€¼ä¹‹ä¸€ | åŸºç¡€ç±»å‹è§£æ |
| RegexParser | ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£æ LLM è¾“å‡º | æ¨¡å¼åŒ¹é…è§£æ |
| RegexDictParser | ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å°†è¾“å‡ºè§£æä¸ºå­—å…¸ | æ¨¡å¼åŒ¹é…è§£æ |
| StructuredOutputParser | å°† LLM è¾“å‡ºè§£æä¸ºç»“æ„åŒ–æ ¼å¼ | ç»“æ„åŒ–è§£æ |
| YamlOutputParser | ä½¿ç”¨ Pydantic æ¨¡å‹è§£æ YAML è¾“å‡º | ç»“æ„åŒ–è§£æ |
| PandasDataFrameOutputParser | ä½¿ç”¨ Pandas DataFrame æ ¼å¼è§£æè¾“å‡º | æ•°æ®å¤„ç†è§£æ |
| CombiningOutputParser | å°†å¤šä¸ªè¾“å‡ºè§£æå™¨ç»„åˆä¸ºä¸€ä¸ª | ç»„åˆè§£æå™¨ |
| OutputFixingParser | åŒ…è£…è§£æå™¨å¹¶å°è¯•ä¿®å¤è§£æé”™è¯¯ | é”™è¯¯å¤„ç†è§£æ |
| RetryOutputParser | åŒ…è£…è§£æå™¨å¹¶å°è¯•ä¿®å¤è§£æé”™è¯¯ | é”™è¯¯å¤„ç†è§£æ |
| RetryWithErrorOutputParser | åŒ…è£…è§£æå™¨å¹¶å°è¯•ä¿®å¤è§£æé”™è¯¯ | é”™è¯¯å¤„ç†è§£æ |
| ResponseSchema | ç»“æ„åŒ–è¾“å‡ºè§£æå™¨çš„å“åº”æ¨¡å¼ | è¾…åŠ©ç±» |

4. å¤æ‚é“¾æ„é€ 

æˆ‘ä»¬ä»¥ä¸€ä¸ªâ€ä»ç¾é£Ÿèœè°±åç§°ç”Ÿæˆåˆ¶ä½œæ­¥éª¤ï¼Œå¹¶ä»ä¸­æå–æ ¸å¿ƒé£Ÿæå’Œçƒ¹é¥ªæ—¶é•¿â€ çš„æ¡ˆä¾‹æ¥è®²è§£ï¼Œè¿™ä¸ªæ¡ˆä¾‹åŒæ ·åŒ…å«äº†â€œæ–‡æœ¬ç”Ÿæˆâ€å’Œâ€œç»“æ„åŒ–ä¿¡æ¯æå–â€ä¸¤ä¸ªç¯èŠ‚ã€‚

```
import os
from langchain.chat_models import init_chat_model
from langchain_core.prompts import PromptTemplate
# è¿™ä¸ªåŒ…åœ¨æ‰€æœ‰ 0.1+ ç‰ˆæœ¬ä¸­éƒ½éå¸¸ç¨³å®š
from langchain_core.output_parsers import JsonOutputParser

# 1. åˆå§‹åŒ–æ¨¡å‹
model = init_chat_model(
    model="doubao-1-5-pro-32k-250115",
    model_provider="openai",
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    api_key=os.environ.get("ARK_OPENAI_API_KEY"),
)

# 2. åˆå§‹åŒ– JsonOutputParser (ä¸éœ€è¦å®šä¹‰å¤æ‚çš„ ResponseSchema)
parser = JsonOutputParser()

# 3. ç¼–å†™é“¾
# ç¬¬ä¸€æ­¥ç”Ÿæˆæ–‡æœ¬
gen_prompt = PromptTemplate.from_template("è¯·æ ¹æ®èœåç¼–å†™ä¸€ä¸ªç®€çŸ­çš„åˆ¶ä½œæ­¥éª¤, åŒ…æ‹¬åˆ¶ä½œéš¾åº¦ä¸è€—æ—¶ï¼š{dish_name}")
# ç¬¬äºŒæ­¥æå–å¹¶å¼ºåˆ¶è¦æ±‚ JSON
extract_prompt = PromptTemplate.from_template(
    "ä»ä»¥ä¸‹å†…å®¹ä¸­æå–ä¿¡æ¯ï¼šé£Ÿææ¸…å•(ingredients)ã€çƒ¹é¥ªæ—¶é•¿(time)ã€éš¾åº¦(difficulty)ã€‚\n"
    "å¿…é¡»ä»¥ JSON æ ¼å¼è¿”å›ã€‚\n"
    "å†…å®¹ï¼š{recipe}"
)

# 4. ç»„åˆé“¾
full_chain = (
    {"recipe": gen_prompt | model | (lambda x: x.content)}
    | extract_prompt
    | model
    | parser  # ç›´æ¥è§£æ JSON
)

# æ‰§è¡Œ
try:
    result = full_chain.invoke({"dish_name": "åœ°ä¸‰é²œ"})
    print(result)
except Exception as e:
    print(f"ä»ç„¶æŠ¥é”™: {e}")
```

### è‡ªå®šä¹‰Langchainç»„ä»¶
Langchain æä¾›äº†å¼€å‘è€…è‡ªå®šä¹‰å¯è¿è¡ŒèŠ‚ç‚¹çš„åŠŸèƒ½.å¦‚æœæˆ‘ä»¬æƒ³åœ¨é“¾ä¸­è®¾ç½®è°ƒè¯•ç»„ä»¶è¯¥å¦‚ä½•ç¼–å†™ä»£ç ?è¿™å°±éœ€è¦ç”¨åˆ°LangChainçš„Runnableç»„ä»¶äº†ã€‚ åœ¨ä¸Šè¿°å¤åˆé“¾ä»£ç ä¸­æ·»åŠ :
```
from langchain_core.runnables import RunnableLambda

def debug_print(x):
    print('ä¸­é—´ç»“æœï¼š', x)
    return x

debug_node = RunnableLambda(debug_print)

# ç»„åˆæˆä¸€ä¸ªå¤åˆ Chain
full_chain = (
    {"recipe": gen_prompt | debug_node | model | (lambda x: x.content)} | debug_node
    | extract_prompt
    | model | debug_node
    | parser  # ç›´æ¥è§£æ JSON
)

# è°ƒç”¨å¤åˆé“¾
result = full_chain.invoke({"title": "åœ°ä¸‰é²œ"})
print(result)

```

è¿è¡Œä¸Šé¢çš„ä»£ç å¯ä»¥çœ‹åˆ°æ¯ä¸€æ­¥éƒ½ä¼šæœ‰ä¸­é—´ç»“æœè¾“å‡ºã€‚


RunnableLambdaå°†pythonå‡½æ•°è½¬æ¢ä¸ºå¯è¿è¡ŒèŠ‚ç‚¹ã€‚è½¬åŒ–åçš„èŠ‚ç‚¹å¯ä»¥åƒä»»ä½•å…¶å®ƒRunnableä¸€æ ·ç»„åˆå¹¶ä¸LangChainé“¾æ— ç¼é›†æˆã€‚ï¼ˆç‰¹åˆ«æ³¨æ„: RunnableLambdaé€‚åˆéæµå¼è¾“å‡ºï¼Œå¦‚æœè¦æµå¼è¾“å‡ºè¯·ä½¿ç”¨RunnableGenerator python.langchain.com/api_referenâ€¦ï¼‰ã€‚

## LCELç®€è¿°
ä»£ç ä¸­|ç¬¦å·è¢«æˆ‘ä»¬å¹¿æ³›ä½¿ç”¨ï¼ŒPythonæ²¡æœ‰è¿™ç§è¯­æ³•ï¼Œä¸ºä»€ä¹ˆè¿™é‡Œå¯ä»¥æŠŠå„ä¸ªç»„ä»¶ä¸²èµ·æ¥ã€‚

å…¶å®è¿™æ˜¯LangChainä¸“é—¨ä¸ºç°ä»£å¤§è¯­è¨€æ¨¡å‹åº”ç”¨å¼€å‘çš„ä¸€ç§å…¨æ–°è¡¨è¾¾èŒƒå¼ï¼Œè¢«ç§°ä¸ºLCELï¼ˆLangChain Expression Languageï¼‰ ã€‚å®ƒä¸ä»…ç®€åŒ–äº†æ¨¡å‹äº¤äº’çš„ç¼–æ’è¿‡ç¨‹ï¼Œè¿˜å¢å¼ºäº†ç»„åˆçš„çµæ´»æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚

###  LCELçš„å®šä¹‰
LCELï¼Œå…¨ç§°ä¸ºLangChain Expression Languageï¼Œæ˜¯ä¸€ç§ä¸“ä¸º LangChain æ¡†æ¶è®¾è®¡çš„è¡¨è¾¾è¯­è¨€ã€‚å®ƒé€šè¿‡ä¸€ç§é“¾å¼ç»„åˆçš„æ–¹å¼ï¼Œå…è®¸å¼€å‘è€…ä½¿ç”¨æ¸…æ™°ã€å£°æ˜å¼çš„è¯­æ³•æ¥æ„å»ºè¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨æµç¨‹ã€‚
ç®€å•æ¥è¯´ï¼ŒLCEL æ˜¯ä¸€ç§â€œå‡½æ•°å¼ç®¡é“é£æ ¼â€çš„ç»„ä»¶ç»„åˆæœºåˆ¶ï¼Œç”¨äºè¿æ¥å„ç§å¯æ‰§è¡Œå•å…ƒï¼ˆRunnableï¼‰ã€‚è¿™äº›å•å…ƒåŒ…æ‹¬æç¤ºæ¨¡æ¿ã€è¯­è¨€æ¨¡å‹ã€è¾“å‡ºè§£æå™¨ã€å·¥å…·å‡½æ•°ç­‰ã€‚

### LCELçš„è®¾è®¡ç›®çš„
LCEL çš„è®¾è®¡åˆè¡·åœ¨äºï¼š

*æ¨¡å—åŒ–æ„å»º*ï¼šå°†æ¨¡å‹è°ƒç”¨æµç¨‹æ‹†è§£ä¸ºç‹¬ç«‹ã€å¯é‡ç”¨çš„ç»„ä»¶ã€‚
*é€»è¾‘å¯è§†åŒ–*ï¼šé€šè¿‡è¯­æ³•ç¬¦å·ï¼ˆå¦‚ç®¡é“ç¬¦ |ï¼‰å‘ˆç°å‡ºæ˜ç¡®çš„æ•°æ®æµè·¯å¾„ã€‚
*ç»Ÿä¸€è¿è¡Œæ¥å£*ï¼šæ‰€æœ‰ LCEL ç»„ä»¶éƒ½å®ç°äº† .invoke()ã€.stream()ã€.batch() ç­‰æ ‡å‡†æ–¹æ³•ï¼Œä¾¿äºåœ¨åŒæ­¥ã€å¼‚æ­¥æˆ–æ‰¹å¤„ç†ç¯å¢ƒä¸‹è°ƒç”¨ã€‚
*è„±ç¦»æ¡†æ¶é™åˆ¶*ï¼šç›¸æ¯”ä¼ ç»Ÿçš„ Chain ç±»å’Œ Agent æ¶æ„ï¼ŒLCEL æ›´è½»é‡ã€æ›´å…·è¡¨è¾¾åŠ›ï¼Œå‡å°‘ä¾èµ–çš„â€œé»‘ç›’â€é€»è¾‘ã€‚

### LCELçš„æ ¸å¿ƒç»„æˆ
LCELçš„æ ¸å¿ƒç»„æˆæœ‰å¦‚ä¸‹ä¸‰ç‚¹:

1. Runnable æ¥å£

LCEL çš„ä¸€åˆ‡åŸºç¡€å•å…ƒéƒ½æ˜¯ Runnable å¯¹è±¡ï¼Œå®ƒæ˜¯ä¸€ç§ç»Ÿä¸€çš„å¯è°ƒç”¨æ¥å£ï¼Œæ”¯æŒå¦‚ä¸‹å½¢å¼ï¼š
.invoke(input)ï¼šåŒæ­¥è°ƒç”¨
.stream(input)ï¼šæµå¼ç”Ÿæˆ
.batch(inputs)ï¼šæ‰¹é‡æ‰§è¡Œ

2. ç®¡é“è¿ç®—ç¬¦ |
è¿™æ˜¯ LCEL æœ€å…·ç‰¹è‰²çš„è¯­æ³•ç¬¦å·ã€‚å¤šä¸ª Runnable å¯¹è±¡(ä¹Ÿå°±æ˜¯æˆ‘ä»¬è¯´çš„ç»„ä»¶)å¯ä»¥é€šè¿‡ | ä¸²è”èµ·æ¥ï¼Œå½¢æˆæ¸…æ™°çš„æ•°æ®å¤„ç†é“¾ã€‚ä¾‹å¦‚ï¼š
```
prompt | model | parser
```

è¡¨ç¤ºæ•°æ®å°†ä¾æ¬¡ä¼ å…¥æç¤ºæ¨¡æ¿ã€æ¨¡å‹å’Œè¾“å‡ºè§£æå™¨ï¼Œæœ€ç»ˆè¾“å‡ºç»“æ„åŒ–ç»“æœã€‚

3. PromptTemplate ä¸ OutputParser
LCEL å¼ºè°ƒç»„ä»¶ä¹‹é—´çš„èŒè´£æ˜ç¡®ï¼ŒPrompt åªè´Ÿè´£æ¨¡æ¿åŒ–è¾“å…¥ï¼ŒParser åªè´Ÿè´£æ ¼å¼åŒ–è¾“å‡ºï¼ŒModel åªè´Ÿè´£æ¨ç†ã€‚

## LangChainè®°å¿†å­˜å‚¨

### LangChainå•è½®ä¼šè¯

ç¼–å†™LangChainå•è½®å¯¹è¯çš„åŸºæœ¬æµç¨‹å¦‚ä¸‹:

1. å¯¼å…¥ç›¸å…³ä¾èµ–åŒ…å¹¶åˆå§‹åŒ–æç¤ºè¯ChatPromptTemplate
2. è°ƒç”¨ç»Ÿä¸€æ¥å£init_chat_modelåˆå§‹åŒ–å¤§æ¨¡å‹ç»„ä»¶
3. ä½¿ç”¨LCELè¯­æ³•å°†å¤§æ¨¡å‹ç»„ä»¶å’Œè¾“å‡ºè§£æå™¨ç»„ä»¶ç›¸è¿æ¥ï¼Œå½¢æˆâ€œé“¾â€
4. æ‰§è¡Œâ€œé“¾â€å¹¶è¾“å‡ºç»“æœ

```
from langchain_core.output_parsers import StrOutputParser
from langchain.chat_models import init_chat_model
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import init_chat_model


chatbot_prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ å«å°æ™ºï¼Œæ˜¯äººå·¥æ™ºèƒ½ä¸“å®¶ã€‚"),
    ("user", "{input}")
])

# ä½¿ç”¨æ¨¡å‹
model = init_chat_model(
    model="Qwen/Qwen3-8B",
    model_provider="openai",
    base_url="",
    api_key="ä½ æ³¨å†Œçš„api key",
)

# ç›´æ¥ä½¿ç”¨æ¨¡å‹ + è¾“å‡ºè§£æå™¨
basic_qa_chain = chatbot_prompt | model | StrOutputParser()

# æµ‹è¯•
question = "ä½ å¥½ï¼Œè¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
result = basic_qa_chain.invoke(question)
print(result)

```

### LangChainå¤šè½®è®°å¿†
è¦æŠŠå•è½®å¯¹è¯ä¿®æ”¹ä¸ºå¤šè½®å¯¹è¯æˆ‘ä»¬åº”è¯¥æ€ä¹ˆåšå‘¢ï¼Ÿé€»è¾‘å…¶å®å¾ˆç®€å•ï¼Œåœ¨LangChainä¸­æˆ‘ä»¬å¯ä»¥é€šè¿‡äººå·¥æ‹¼æ¥æ¶ˆæ¯é˜Ÿåˆ—æ¥ä¸ºæ¯æ¬¡æ¨¡å‹è°ƒç”¨è®¾ç½®å¤šè½®å¯¹è¯è®°å¿†ã€‚éœ€è¦è¿›è¡Œå¦‚ä¸‹æ­¥éª¤ï¼š

1. æ„å»ºæç¤ºè¯ç»„ä»¶ChatPromptTemplateæ—¶ï¼Œé€šè¿‡å ä½ç¬¦MessagePlaceholderå®šä¹‰ä¸€ä¸ªæ¶ˆæ¯åˆ—è¡¨, å…³é”®ä»£ç ä¸ºï¼š
```
prompt = ChatPromptTemplate.from_messages([ 
    SystemMessage(content="ä½ å«å°æ™ºï¼Œæ˜¯äººå·¥æ™ºèƒ½ä¸“å®¶ã€‚"),  
    MessagesPlaceholder(variable_name="messages"), 
])
```

2. åœ¨å¤šè½®å¯¹è¯ä¸­ä¸æ–­çš„å‘messageåˆ—è¡¨ä¸­è¿½åŠ æ¶ˆæ¯ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™å ä½ç¬¦ï¼Œå¤§æ¨¡å‹ç»„ä»¶æ¥æ”¶åˆ°åˆ—è¡¨ä¿¡æ¯åä¼šè‡ªåŠ¨å…³è”å†å²æ¶ˆæ¯å¹¶å›å¤å†…å®¹, å…³é”®ä»£ç ä¸º:

```
messages_list.append(HumanMessage(content=user_query)) 

assistant_reply = chain.invoke({"messages": messages_list}) 

print("å°æ™ºï¼š", assistant_reply)
```

å®Œæ•´çš„å¤šè½®å¯¹è¯ä»£ç å¦‚ä¸‹:

```
import os
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chat_models import init_chat_model
from langchain_core.output_parsers import StrOutputParser

# ä½¿ç”¨ ç¡…åŸºæµåŠ¨ æ¨¡å‹
model = init_chat_model(
    model="doubao-1-5-pro-32k-250115",
    model_provider="openai",
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    api_key=os.environ.get("ARK_OPENAI_API_KEY"),
)

parser = StrOutputParser()

prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content="ä½ å«å°æ™ºï¼Œæ˜¯äººå·¥æ™ºèƒ½ä¸“å®¶ã€‚"),
    MessagesPlaceholder(variable_name="messages"),
])

chain = prompt | model | parser

messages_list = []  # åˆå§‹åŒ–å†å²
print("ğŸ”¹ è¾“å…¥ exit ç»“æŸå¯¹è¯")
while True:
    user_query = input("ä½ ï¼š")
    if user_query.lower() in {"exit", "quit"}:
        break
    messages_list.append(HumanMessage(content=user_query))
    assistant_reply = chain.invoke({"messages": messages_list})
    print("å°æ™ºï¼š", assistant_reply)
    messages_list.append(AIMessage(content=assistant_reply))
    messages_list = messages_list[-50:]
```

### æµå¼æ‰“å°
LangChainæä¾›äº†ä¸€ä¸ªstreamæ–¹æ³•ï¼Œå¯ä»¥å®ç°æµå¼è¾“å‡ºï¼Œåªéœ€è¦åœ¨è°ƒç”¨æ¨¡å‹å›ç­”æ—¶å°†invokeæ–¹æ³•æ›¿æ¢ä¸ºstreamå³å¯ã€‚stream()æ˜¯åŒæ­¥æ–¹æ³•,ä½¿ç”¨forå¾ªç¯æ¥å—è¿”å›çš„chunkå—ã€‚å¦‚æœå¼‚æ­¥è°ƒç”¨ï¼Œéœ€è¦ä½¿ç”¨astream(),ç„¶åä½¿ç”¨async forå¼‚æ­¥forå¾ªç¯è·å–æ¨¡å‹è¾“å‡ºã€‚

```
#  è°ƒç”¨æ¨¡å‹
assistant_reply=''
print('å°æ™º:', end=' ')
for chunk in chain.stream({"messages": messages_list}):
    assistant_reply+=chunk
    print(chunk, end="", flush=True)
print()

# è¿½åŠ  AI å›å¤
messages_list.append(AIMessage(content=assistant_reply))

```
